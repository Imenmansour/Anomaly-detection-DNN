{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eedf9049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab42110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SafeML path found: C:\\Users\\G800613RTS\\Desktop\\Det_Ano_IM\\SafeML\\Implementation_in_Python\n",
      "✅ All SafeML modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the correct path to the SafeML Implementation_in_Python folder\n",
    "sys.path.insert(0, r'C:\\Users\\G800613RTS\\Desktop\\Anomaly\\SafeML\\Implementation_in_Python')\n",
    "\n",
    "# Alternative relative path (if running from the Anomaly folder)\n",
    "# sys.path.insert(0, './SafeML/Implementation_in_Python')\n",
    "\n",
    "# Verify the path exists\n",
    "safeml_path = r'C:\\Users\\G800613RTS\\Desktop\\Anomaly\\SafeML\\Implementation_in_Python'\n",
    "if os.path.exists(safeml_path):\n",
    "    print(f\"✅ SafeML path found: {safeml_path}\")\n",
    "else:\n",
    "    print(f\"❌ SafeML path NOT found: {safeml_path}\")\n",
    "    print(\"Current working directory:\", os.getcwd())\n",
    "    print(\"Available directories:\", os.listdir('.'))\n",
    "\n",
    "# Importing local modules (statistical distance measures)\n",
    "from CVM_Distance import CVM_Dist as Cramer_Von_Mises_Dist\n",
    "from Anderson_Darling_Distance import Anderson_Darling_Dist\n",
    "from Kolmogorov_Smirnov_Distance import Kolmogorov_Smirnov_Dist\n",
    "from KuiperDistance import Kuiper_Dist\n",
    "from WassersteinDistance import Wasserstein_Dist\n",
    "from DTS_Distance import DTS_Dist # Combo of Anderson_Darling and CVM distance.\n",
    "\n",
    "print(\"✅ All SafeML modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83fbf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b533542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os, sys # For accessing Python Modules in the System Path (for accessing the Statistical Measures modules)\n",
    "# See: https://stackoverflow.com/a/39311677\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "import pandas as pd # For DataFrames, Series, and reading csv data in.\n",
    "import seaborn as sns # Graphing, built ontop of MatPlot for ease-of-use and nicer diagrams.\n",
    "import matplotlib.pyplot as plt # MatPlotLib for graphing data visually. Seaborn more likely to be used.\n",
    "import numpy as np # For manipulating arrays and changing data into correct formats for certain libraries\n",
    "import sklearn # For Machine Learning algorithms\n",
    "\n",
    "from sklearn.decomposition import PCA # For PCA dimensionality reduction technique\n",
    "from sklearn.preprocessing import StandardScaler # For scaling to unit scale, before PCA application\n",
    "from sklearn.preprocessing import LabelBinarizer # For converting categorical data into numeric, for modeling stage\n",
    "from sklearn.model_selection import StratifiedKFold # For optimal train_test splitting, for model input data\n",
    "from sklearn.model_selection import train_test_split # For basic dataset splitting\n",
    "from sklearn.neighbors import KNeighborsClassifier # K-Nearest Neighbors ML classifier (default n. of neighbors = 5)\n",
    "\n",
    "from sklearn.metrics import accuracy_score # For getting the accuracy of a model's predictions\n",
    "from sklearn.metrics import classification_report # Various metrics for model performance\n",
    "from sklearn.neural_network import MLPClassifier # For Neural Network classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow/Keras imports for DNN model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9ba3b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df):\n",
    "    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "    df.dropna(inplace=True)\n",
    "    # Fix: Use axis parameter instead of positional argument\n",
    "    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(axis=1)\n",
    "    return df[indices_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3850ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Custom confusion matrix plotting function using matplotlib and seaborn\n",
    "    \"\"\"\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Normal (0)', 'Anomaly (1)'], \n",
    "                yticklabels=['Normal (0)', 'Anomaly (1)'],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    \n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print confusion matrix statistics\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    total = cm.sum()\n",
    "    accuracy = (tp + tn) / total\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    \n",
    "    print(f\"📊 Confusion Matrix Statistics:\")\n",
    "    print(f\"   True Negatives (TN):  {tn:4d}\")\n",
    "    print(f\"   False Positives (FP): {fp:4d}\")\n",
    "    print(f\"   False Negatives (FN): {fn:4d}\")\n",
    "    print(f\"   True Positives (TP):  {tp:4d}\")\n",
    "    print(f\"   Accuracy:    {accuracy:.4f}\")\n",
    "    print(f\"   Precision:   {precision:.4f}\")\n",
    "    print(f\"   Recall:      {recall:.4f}\")\n",
    "    print(f\"   Specificity: {specificity:.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3966bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_DNN_model(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train a Deep Neural Network using TensorFlow/Keras with early stopping.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, X_test : pandas.DataFrame or numpy.array\n",
    "        Training and test feature data\n",
    "    y_train, y_test : pandas.Series or numpy.array\n",
    "        Training and test labels\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (predictions, accuracy, model, scaler)\n",
    "        Model predictions, accuracy score, trained model, and fitted scaler\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense\n",
    "    from tensorflow.keras.metrics import AUC\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import numpy as np\n",
    "    \n",
    "    # Scale the data for neural network\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Build the DNN model\n",
    "    dnn_model = Sequential([\n",
    "        Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    dnn_model.compile(\n",
    "        optimizer='adam', \n",
    "        loss='binary_crossentropy', \n",
    "        metrics=['accuracy', AUC()]\n",
    "    )\n",
    "    \n",
    "    # Add Early Stopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=0,  # Reduced verbosity for cleaner output\n",
    "        min_delta=0.001\n",
    "    )\n",
    "    \n",
    "    # Train the model with scaled data and early stopping\n",
    "    history = dnn_model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0  # Reduced verbosity for cleaner output\n",
    "    )\n",
    "    print(f\"\\n✅ Training completed! Stopped at epoch {len(history.history['loss'])}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    pred_y_prob = dnn_model.predict(X_test_scaled, verbose=0)\n",
    "    pred_y = (pred_y_prob > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, pred_y)\n",
    "    \n",
    "    # Return predictions, accuracy, trained model, and fitted scaler\n",
    "    return pred_y, accuracy, dnn_model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fe4212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_train_and_test_data_for_given_label(labels, label_index, pred_y, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Get training and test data for a specific label/class.\n",
    "    Works with numpy arrays from train_test_split.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    labels : array\n",
    "        Unique labels in the dataset\n",
    "    label_index : int\n",
    "        Index of the label to filter for\n",
    "    pred_y : array\n",
    "        Predicted labels\n",
    "    X_train, X_test : numpy.ndarray\n",
    "        Training and test feature data (numpy arrays)\n",
    "    y_train, y_test : numpy.ndarray\n",
    "        Training and test labels (numpy arrays)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (X_train_for_label, X_test_for_label) as pandas DataFrames\n",
    "    \"\"\"\n",
    "    # Since X_train, X_test, y_train are numpy arrays, use boolean indexing\n",
    "    train_mask = y_train == labels[label_index]\n",
    "    test_mask = pred_y == labels[label_index]\n",
    "    \n",
    "    # Filter the data using boolean masks\n",
    "    X_train_loc_for_label = X_train[train_mask]\n",
    "    X_test_loc_for_label = X_test[test_mask]\n",
    "    \n",
    "    # Convert to pandas DataFrames with the correct feature names\n",
    "    # These are the 10 selected features from your df_new\n",
    "    feature_columns = ['dst_port','flow_duration', 'fwd_pkt_len_max', 'fwd_pkt_len_mean',\n",
    "       'pkt_len_mean', 'pkt_len_std', 'fwd_iat_tot', 'syn_flag_cnt',\n",
    "       'pkt_size_avg', 'fwd_seg_size_avg']\n",
    "    \n",
    "    X_train_loc_for_label = pd.DataFrame(X_train_loc_for_label, columns=feature_columns)\n",
    "    X_test_loc_for_label = pd.DataFrame(X_test_loc_for_label, columns=feature_columns)\n",
    "    \n",
    "    return X_train_loc_for_label, X_test_loc_for_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63d440ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_statistical_dist_measures_for_class_result(accuracy, X_train_L, X_test_L):\n",
    "    \"\"\"\n",
    "    Calculate statistical distance measures for a specific class result including accuracy.\n",
    "    Used during training phase to build the statistical distance database.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    accuracy : float\n",
    "        Model accuracy for this training run\n",
    "    X_train_L : pandas.DataFrame\n",
    "        Training data for a specific class/label\n",
    "    X_test_L : pandas.DataFrame  \n",
    "        Test data for the same class/label\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing accuracy and statistical distance measures\n",
    "    \"\"\"\n",
    "    # Check if either dataset is empty\n",
    "    if len(X_train_L) == 0 or len(X_test_L) == 0:\n",
    "        print(f\"Warning: Empty dataset detected. Train size: {len(X_train_L)}, Test size: {len(X_test_L)}\")\n",
    "        return {'Accuracy': accuracy,\n",
    "                'Anderson_Darling_dist': np.nan,\n",
    "                'CVM_dist': np.nan,\n",
    "                'DTS_dist': np.nan,\n",
    "                'Kolmogorov_Smirnov_dist': np.nan,\n",
    "                'Kuiper_dist': np.nan,\n",
    "                'Wasserstein_dist': np.nan}\n",
    "    \n",
    "    num_of_features = len(X_train_L.columns)\n",
    "    \n",
    "    # Instantiate empty arrays with large enough size, to hold statistical distance data\n",
    "    CVM_distances = np.zeros(num_of_features)\n",
    "    Anderson_Darling_distances = np.zeros(num_of_features)\n",
    "    Kolmogorov_Smirnov_distances = np.zeros(num_of_features)\n",
    "    Kuiper_distances = np.zeros(num_of_features)\n",
    "    Wasserstein_distances = np.zeros(num_of_features)\n",
    "    DTS_distances = np.zeros(num_of_features)\n",
    "\n",
    "    for i in range(0, num_of_features):\n",
    "        # iloc[:, i] allows selection of the ith feature in the Pandas dataframe\n",
    "        # Calling the methods from the imported Python modules (see import section at top of notebook)\n",
    "        CVM_distances[i] = Cramer_Von_Mises_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n",
    "        Anderson_Darling_distances[i] = Anderson_Darling_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n",
    "        Kolmogorov_Smirnov_distances[i] = Kolmogorov_Smirnov_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n",
    "        Kuiper_distances[i] = Kuiper_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n",
    "        Wasserstein_distances[i] = Wasserstein_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n",
    "        DTS_distances[i] = DTS_Dist(X_train_L.iloc[:, i], X_test_L.iloc[:, i])\n",
    "        \n",
    "    # Computing mean/ average, to get ECDF distance of full dataset. Float64 to keep accuracy high.\n",
    "    # See: https://numpy.org/doc/stable/reference/generated/numpy.mean.html\n",
    "    CVM_distance = np.mean(CVM_distances, dtype=np.float64)\n",
    "    Anderson_Darling_distance = np.mean(Anderson_Darling_distances, dtype=np.float64)\n",
    "    Kolmogorov_Smirnov_distance = np.mean(Kolmogorov_Smirnov_distances, dtype=np.float64)\n",
    "    Kuiper_distance = np.mean(Kuiper_distances, dtype=np.float64)\n",
    "    Wasserstein_distance = np.mean(Wasserstein_distances, dtype=np.float64)\n",
    "    DTS_distance = np.mean(DTS_distances, dtype=np.float64)\n",
    "    \n",
    "    # Returning dictionary, for efficient and fast DataFrame creation. Returns mean for each distance.\n",
    "    # See https://stackoverflow.com/a/17496530. Fast way to 'append' to dataframe for results table.\n",
    "    # PRESERVE THE ORDERING\n",
    "    return {'Accuracy': accuracy,\n",
    "            'Anderson_Darling_dist': Anderson_Darling_distance,\n",
    "            'CVM_dist': CVM_distance,\n",
    "            'DTS_dist':DTS_distance,\n",
    "            'Kolmogorov_Smirnov_dist':Kolmogorov_Smirnov_distance,\n",
    "            'Kuiper_dist': Kuiper_distance,\n",
    "            'Wasserstein_dist': Wasserstein_distance}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6693a",
   "metadata": {},
   "source": [
    "Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624d8ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DOS folder found: C:\\Users\\G800613RTS\\Desktop\\Det_Ano_IM\\ddos\n",
      "Files in folder: ['dos.csv', 'dos1.csv', 'dos2.csv', 'dos3.csv', 'dos4.csv', 'dos5.csv', 'dos6.csv', 'dos7.csv', 'dos8.csv']\n",
      "📁 Reading dos.csv: 15662 total rows\n",
      "✅ Loaded dos.csv: 15502 rows with label=1 and target ports\n",
      "📁 Reading dos1.csv: 15736 total rows\n",
      "✅ Loaded dos1.csv: 15520 rows with label=1 and target ports\n",
      "📁 Reading dos2.csv: 77 total rows\n",
      "✅ Loaded dos2.csv: 4 rows with label=1 and target ports\n",
      "📁 Reading dos3.csv: 70 total rows\n",
      "✅ Loaded dos3.csv: 4 rows with label=1 and target ports\n",
      "📁 Reading dos4.csv: 37 total rows\n",
      "✅ Loaded dos4.csv: 1 rows with label=1 and target ports\n",
      "📁 Reading dos5.csv: 590 total rows\n",
      "✅ Loaded dos5.csv: 501 rows with label=1 and target ports\n",
      "📁 Reading dos6.csv: 2413 total rows\n",
      "✅ Loaded dos6.csv: 2309 rows with label=1 and target ports\n",
      "📁 Reading dos7.csv: 947 total rows\n",
      "✅ Loaded dos7.csv: 893 rows with label=1 and target ports\n",
      "📁 Reading dos8.csv: 2011 total rows\n",
      "✅ Loaded dos8.csv: 1947 rows with label=1 and target ports\n",
      "\n",
      "🎯 Total rows with label=1 and target ports: 36681\n",
      "\n",
      "==================================================\n",
      "ORIGINAL DESTINATION PORT ANALYSIS\n",
      "==================================================\n",
      "Original distribution:\n",
      "dst_port\n",
      "8443    17414\n",
      "8008    15995\n",
      "9000     2308\n",
      "8009      964\n",
      "Name: count, dtype: int64\n",
      "\n",
      "==================================================\n",
      "APPLYING CUSTOM SAMPLING\n",
      "==================================================\n",
      "Port 8009: 964 → 964 rows (100%)\n",
      "Port 8008: 15995 → 4798 rows (30%)\n",
      "Port 8443: 17414 → 5224 rows (30%)\n",
      "Port 9000: 2308 → 692 rows (30%)\n",
      "\n",
      "==================================================\n",
      "FINAL DESTINATION PORT ANALYSIS\n",
      "==================================================\n",
      "Number of unique destination ports: 4\n",
      "\n",
      "Unique destination ports found:\n",
      "[8008, 8009, 8443, 9000]\n",
      "\n",
      "Final port distribution:\n",
      "dst_port\n",
      "8443    5224\n",
      "8008    4798\n",
      "8009     964\n",
      "9000     692\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Destination port statistics:\n",
      "Min port: 8008\n",
      "Max port: 9000\n",
      "Mean port: 8261.46\n",
      "\n",
      "Final ports percentage distribution:\n",
      "Port 8443: 44.73%\n",
      "Port 8008: 41.09%\n",
      "Port 8009: 8.25%\n",
      "Port 9000: 5.93%\n",
      "\n",
      "==================================================\n",
      "SAMPLING SUMMARY\n",
      "==================================================\n",
      "Total rows after sampling: 11678\n",
      "Target ports NOT found in data: [443]\n",
      "\n",
      "📊 Final dataset shape: (11678, 83)\n",
      "📊 Dataset columns: ['src_ip', 'dst_ip', 'src_port', 'dst_port', 'protocol', 'timestamp', 'flow_duration', 'flow_byts_s', 'flow_pkts_s', 'fwd_pkts_s', 'bwd_pkts_s', 'tot_fwd_pkts', 'tot_bwd_pkts', 'totlen_fwd_pkts', 'totlen_bwd_pkts', 'fwd_pkt_len_max', 'fwd_pkt_len_min', 'fwd_pkt_len_mean', 'fwd_pkt_len_std', 'bwd_pkt_len_max', 'bwd_pkt_len_min', 'bwd_pkt_len_mean', 'bwd_pkt_len_std', 'pkt_len_max', 'pkt_len_min', 'pkt_len_mean', 'pkt_len_std', 'pkt_len_var', 'fwd_header_len', 'bwd_header_len', 'fwd_seg_size_min', 'fwd_act_data_pkts', 'flow_iat_mean', 'flow_iat_max', 'flow_iat_min', 'flow_iat_std', 'fwd_iat_tot', 'fwd_iat_max', 'fwd_iat_min', 'fwd_iat_mean', 'fwd_iat_std', 'bwd_iat_tot', 'bwd_iat_max', 'bwd_iat_min', 'bwd_iat_mean', 'bwd_iat_std', 'fwd_psh_flags', 'bwd_psh_flags', 'fwd_urg_flags', 'bwd_urg_flags', 'fin_flag_cnt', 'syn_flag_cnt', 'rst_flag_cnt', 'psh_flag_cnt', 'ack_flag_cnt', 'urg_flag_cnt', 'ece_flag_cnt', 'down_up_ratio', 'pkt_size_avg', 'init_fwd_win_byts', 'init_bwd_win_byts', 'active_max', 'active_min', 'active_mean', 'active_std', 'idle_max', 'idle_min', 'idle_mean', 'idle_std', 'fwd_byts_b_avg', 'fwd_pkts_b_avg', 'bwd_byts_b_avg', 'bwd_pkts_b_avg', 'fwd_blk_rate_avg', 'bwd_blk_rate_avg', 'fwd_seg_size_avg', 'bwd_seg_size_avg', 'cwr_flag_count', 'subflow_fwd_pkts', 'subflow_bwd_pkts', 'subflow_fwd_byts', 'subflow_bwd_byts', 'Label']\n",
      "📊 Label distribution: {1: 11678}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the folder path where DOS files are located\n",
    "dos_folder_path = r'C:\\Users\\G800613RTS\\Desktop\\Anomaly\\ddos'\n",
    "\n",
    "# List of DOS files\n",
    "dos_files = ['dos.csv', 'dos1.csv', 'dos2.csv', 'dos3.csv', 'dos4.csv', \n",
    "             'dos5.csv', 'dos6.csv', 'dos7.csv', 'dos8.csv']\n",
    "\n",
    "# Specific destination ports to filter for\n",
    "target_ports = [8008, 8009, 8443, 9000, 443]\n",
    "\n",
    "# Verify the folder exists\n",
    "if not os.path.exists(dos_folder_path):\n",
    "    print(f\"❌ DOS folder not found: {dos_folder_path}\")\n",
    "    print(\"Please verify the folder path exists\")\n",
    "else:\n",
    "    print(f\"✅ DOS folder found: {dos_folder_path}\")\n",
    "    print(f\"Files in folder: {os.listdir(dos_folder_path)}\")\n",
    "\n",
    "# Read all files and filter for label=1 AND specific destination ports\n",
    "filtered_data = []\n",
    "\n",
    "for file in dos_files:\n",
    "    try:\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(dos_folder_path, file)\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"❌ File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"📁 Reading {file}: {len(df)} total rows\")\n",
    "        \n",
    "        # Check if required columns exist\n",
    "        if 'Label' not in df.columns:\n",
    "            print(f\"⚠️ No 'Label' column found in {file}\")\n",
    "            continue\n",
    "        if 'dst_port' not in df.columns:\n",
    "            print(f\"⚠️ No 'dst_port' column found in {file}\")\n",
    "            continue\n",
    "        \n",
    "        # Filter rows where label equals 1 AND dst_port is in target_ports\n",
    "        filtered_df = df[(df['Label'] == 1) & (df['dst_port'].isin(target_ports))]\n",
    "        \n",
    "        # Add to our list if we have data\n",
    "        if len(filtered_df) > 0:\n",
    "            filtered_data.append(filtered_df)\n",
    "            print(f\"✅ Loaded {file}: {len(filtered_df)} rows with label=1 and target ports\")\n",
    "        else:\n",
    "            print(f\"⚠️ No data found in {file} with label=1 and target ports\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ File {file} not found at path: {os.path.join(dos_folder_path, file)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading {file}: {e}\")\n",
    "\n",
    "# Combine all filtered data into one DataFrame\n",
    "if filtered_data:\n",
    "    combined_df = pd.concat(filtered_data, ignore_index=True)\n",
    "    print(f\"\\n🎯 Total rows with label=1 and target ports: {len(combined_df)}\")\n",
    "    \n",
    "    # Analyze destination ports BEFORE sampling\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ORIGINAL DESTINATION PORT ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    dst_port_counts = combined_df['dst_port'].value_counts()\n",
    "    print(\"Original distribution:\")\n",
    "    print(dst_port_counts)\n",
    "    \n",
    "    # === APPLY CUSTOM SAMPLING BY PORT ===\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"APPLYING CUSTOM SAMPLING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    sampled_data = []\n",
    "    \n",
    "    for port in combined_df['dst_port'].unique():\n",
    "        port_data = combined_df[combined_df['dst_port'] == port]\n",
    "        original_count = len(port_data)\n",
    "        \n",
    "        if port == 9000:\n",
    "            # Take 30% of port 9000 data\n",
    "            sample_fraction = 0.3\n",
    "            sampled_port_data = port_data.sample(frac=sample_fraction, random_state=42)\n",
    "            print(f\"Port {port}: {original_count} → {len(sampled_port_data)} rows (30%)\")\n",
    "            \n",
    "        elif port == 8443:\n",
    "            # Take 30% of port 8443 data\n",
    "            sample_fraction = 0.3\n",
    "            sampled_port_data = port_data.sample(frac=sample_fraction, random_state=42)\n",
    "            print(f\"Port {port}: {original_count} → {len(sampled_port_data)} rows (30%)\")\n",
    "            \n",
    "        elif port == 8008:\n",
    "            # Take 30% of port 8008 data\n",
    "            sample_fraction = 0.3\n",
    "            sampled_port_data = port_data.sample(frac=sample_fraction, random_state=42)\n",
    "            print(f\"Port {port}: {original_count} → {len(sampled_port_data)} rows (30%)\")\n",
    "            \n",
    "        else:\n",
    "            # Keep all data for other ports (8009, 443)\n",
    "            sampled_port_data = port_data\n",
    "            print(f\"Port {port}: {original_count} → {len(sampled_port_data)} rows (100%)\")\n",
    "        \n",
    "        sampled_data.append(sampled_port_data)\n",
    "    \n",
    "    # Combine sampled data\n",
    "    combined_df = pd.concat(sampled_data, ignore_index=True)\n",
    "    \n",
    "    # Analyze destination ports AFTER sampling\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"FINAL DESTINATION PORT ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get unique destination ports\n",
    "    unique_dst_ports = combined_df['dst_port'].unique()\n",
    "    print(f\"Number of unique destination ports: {len(unique_dst_ports)}\")\n",
    "    \n",
    "    # Show all unique destination ports (sorted)\n",
    "    print(f\"\\nUnique destination ports found:\")\n",
    "    print(sorted(unique_dst_ports))\n",
    "    \n",
    "    # Show distribution of target ports after sampling\n",
    "    print(f\"\\nFinal port distribution:\")\n",
    "    dst_port_counts_final = combined_df['dst_port'].value_counts()\n",
    "    print(dst_port_counts_final)\n",
    "    \n",
    "    # Show port statistics\n",
    "    print(f\"\\nDestination port statistics:\")\n",
    "    print(f\"Min port: {combined_df['dst_port'].min()}\")\n",
    "    print(f\"Max port: {combined_df['dst_port'].max()}\")\n",
    "    print(f\"Mean port: {combined_df['dst_port'].mean():.2f}\")\n",
    "    \n",
    "    # Show percentage distribution of target ports\n",
    "    print(f\"\\nFinal ports percentage distribution:\")\n",
    "    dst_port_percentages = combined_df['dst_port'].value_counts(normalize=True) * 100\n",
    "    for port, percentage in dst_port_percentages.items():\n",
    "        print(f\"Port {port}: {percentage:.2f}%\")\n",
    "    \n",
    "    # Summary of changes\n",
    "    print(f\"\\n\" + \"=\"*50)\n",
    "    print(\"SAMPLING SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Total rows after sampling: {len(combined_df)}\")\n",
    "    \n",
    "    # Show which target ports were found vs not found\n",
    "    found_ports = set(unique_dst_ports)\n",
    "    missing_ports = set(target_ports) - found_ports\n",
    "    \n",
    "    if missing_ports:\n",
    "        print(f\"Target ports NOT found in data: {sorted(missing_ports)}\")\n",
    "    else:\n",
    "        print(f\"All target ports found in data!\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No data found with the specified criteria\")\n",
    "    combined_df = pd.DataFrame()\n",
    "\n",
    "# Store the filtered and sampled data for further use\n",
    "df_data = combined_df\n",
    "\n",
    "print(f\"\\n📊 Final dataset shape: {df_data.shape}\")\n",
    "if len(df_data) > 0:\n",
    "    print(f\"📊 Dataset columns: {list(df_data.columns)}\")\n",
    "    print(f\"📊 Label distribution: {df_data['Label'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b53426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Session folder found: C:\\Users\\G800613RTS\\Desktop\\Det_Ano_IM\\normal\n",
      "Files in folder: ['output.csv', 'output1.csv', 'session10.csv', 'session12.csv', 'session13.csv', 'session15.csv', 'session3.csv', 'session5.csv', 'session7.csv', 'session8.csv', 'session9.csv']\n",
      "✅ Successfully loaded output.csv with 9605 rows and 83 columns\n",
      "✅ Successfully loaded output1.csv with 4611 rows and 83 columns\n",
      "✅ Successfully loaded session3.csv with 4611 rows and 83 columns\n",
      "✅ Successfully loaded session5.csv with 1066 rows and 83 columns\n",
      "✅ Successfully loaded session7.csv with 2686 rows and 83 columns\n",
      "✅ Successfully loaded session8.csv with 3754 rows and 83 columns\n",
      "✅ Successfully loaded session9.csv with 3808 rows and 83 columns\n",
      "✅ Successfully loaded session10.csv with 1892 rows and 83 columns\n",
      "✅ Successfully loaded session12.csv with 3150 rows and 83 columns\n",
      "✅ Successfully loaded session13.csv with 4690 rows and 83 columns\n",
      "✅ Successfully loaded session15.csv with 282 rows and 83 columns\n",
      "\n",
      "======================================================================\n",
      "SESSION/OUTPUT FILES CONCATENATION RESULTS:\n",
      "======================================================================\n",
      "✅ Total files processed: 11\n",
      "📊 Concatenated dataset shape: (40155, 83)\n",
      "📈 Total rows: 40,155\n",
      "📋 Total columns: 83\n",
      "\n",
      "📁 Successfully loaded files:\n",
      "\n",
      "📊 Dataset Overview:\n",
      "   Column names: ['src_ip', 'dst_ip', 'src_port', 'dst_port', 'protocol', 'timestamp', 'flow_duration', 'flow_byts_s', 'flow_pkts_s', 'fwd_pkts_s', 'bwd_pkts_s', 'tot_fwd_pkts', 'tot_bwd_pkts', 'totlen_fwd_pkts', 'totlen_bwd_pkts', 'fwd_pkt_len_max', 'fwd_pkt_len_min', 'fwd_pkt_len_mean', 'fwd_pkt_len_std', 'bwd_pkt_len_max', 'bwd_pkt_len_min', 'bwd_pkt_len_mean', 'bwd_pkt_len_std', 'pkt_len_max', 'pkt_len_min', 'pkt_len_mean', 'pkt_len_std', 'pkt_len_var', 'fwd_header_len', 'bwd_header_len', 'fwd_seg_size_min', 'fwd_act_data_pkts', 'flow_iat_mean', 'flow_iat_max', 'flow_iat_min', 'flow_iat_std', 'fwd_iat_tot', 'fwd_iat_max', 'fwd_iat_min', 'fwd_iat_mean', 'fwd_iat_std', 'bwd_iat_tot', 'bwd_iat_max', 'bwd_iat_min', 'bwd_iat_mean', 'bwd_iat_std', 'fwd_psh_flags', 'bwd_psh_flags', 'fwd_urg_flags', 'bwd_urg_flags', 'fin_flag_cnt', 'syn_flag_cnt', 'rst_flag_cnt', 'psh_flag_cnt', 'ack_flag_cnt', 'urg_flag_cnt', 'ece_flag_cnt', 'down_up_ratio', 'pkt_size_avg', 'init_fwd_win_byts', 'init_bwd_win_byts', 'active_max', 'active_min', 'active_mean', 'active_std', 'idle_max', 'idle_min', 'idle_mean', 'idle_std', 'fwd_byts_b_avg', 'fwd_pkts_b_avg', 'bwd_byts_b_avg', 'bwd_pkts_b_avg', 'fwd_blk_rate_avg', 'bwd_blk_rate_avg', 'fwd_seg_size_avg', 'bwd_seg_size_avg', 'cwr_flag_count', 'subflow_fwd_pkts', 'subflow_bwd_pkts', 'subflow_fwd_byts', 'subflow_bwd_byts', 'Label']\n",
      "   Label distribution:\n",
      "     Label 0: 40,155 rows (100.0%)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the folder path where session/output files are located\n",
    "session_folder_path = r'C:\\Users\\G800613RTS\\Desktop\\Anomaly\\normal'\n",
    "\n",
    "# List of all session/output CSV files\n",
    "session_files = ['output.csv', 'output1.csv', 'session3.csv', 'session5.csv', 'session7.csv', \n",
    "                 'session8.csv', 'session9.csv', 'session10.csv', 'session12.csv', 'session13.csv', 'session15.csv']\n",
    "\n",
    "# Verify the folder exists\n",
    "if not os.path.exists(session_folder_path):\n",
    "    print(f\"❌ Session folder not found: {session_folder_path}\")\n",
    "    print(\"Please verify the folder path exists\")\n",
    "else:\n",
    "    print(f\"✅ Session folder found: {session_folder_path}\")\n",
    "    print(f\"Files in folder: {os.listdir(session_folder_path)}\")\n",
    "\n",
    "dfs_session = []\n",
    "\n",
    "# Load all session files into a list\n",
    "for file_name in session_files:\n",
    "    try:\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(session_folder_path, file_name)\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"❌ File not found: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        dfs_session.append(df)\n",
    "        print(f\"✅ Successfully loaded {file_name} with {len(df)} rows and {len(df.columns)} columns\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ File {file_name} not found at path: {os.path.join(session_folder_path, file_name)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {file_name}: {e}\")\n",
    "\n",
    "# Check if any files were loaded successfully\n",
    "if dfs_session:\n",
    "    # Concatenate all session DataFrames into a single DataFrame\n",
    "    df_session_data = pd.concat(dfs_session, axis=0, ignore_index=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SESSION/OUTPUT FILES CONCATENATION RESULTS:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"✅ Total files processed: {len(dfs_session)}\")\n",
    "    print(f\"📊 Concatenated dataset shape: {df_session_data.shape}\")\n",
    "    print(f\"📈 Total rows: {len(df_session_data):,}\")\n",
    "    print(f\"📋 Total columns: {len(df_session_data.columns)}\")\n",
    "    \n",
    "    # Show which files were successfully loaded\n",
    "    print(f\"\\n📁 Successfully loaded files:\")\n",
    "    for i, file_name in enumerate([f for f in session_files if any(f in str(df) for df in dfs_session)]):\n",
    "        print(f\"   {i+1}. {file_name}\")\n",
    "    \n",
    "    # Show basic dataset info\n",
    "    if len(df_session_data) > 0:\n",
    "        print(f\"\\n📊 Dataset Overview:\")\n",
    "        print(f\"   Column names: {list(df_session_data.columns)}\")\n",
    "        \n",
    "        # Check for Label column and show distribution if it exists\n",
    "        if 'Label' in df_session_data.columns:\n",
    "            label_counts = df_session_data['Label'].value_counts()\n",
    "            print(f\"   Label distribution:\")\n",
    "            for label, count in label_counts.items():\n",
    "                print(f\"     Label {label}: {count:,} rows ({count/len(df_session_data)*100:.1f}%)\")\n",
    "        else:\n",
    "            print(\"   No 'Label' column found in the data\")\n",
    "            \n",
    "else:\n",
    "    print(\"❌ No session files were loaded successfully\")\n",
    "    df_session_data = pd.DataFrame()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0048641f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df_data, df_session_data], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83a7af29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMBINED DATASET RESULTS:\n",
      "======================================================================\n",
      "df_data (DOS files) shape: (11678, 83)\n",
      "df_session_data (Session files) shape: (40155, 83)\n",
      "Combined dataset shape: (51833, 83)\n",
      "\n",
      "Total rows in combined dataset: 51833\n",
      "Total columns in combined dataset: 83\n",
      "\n",
      "==================================================\n",
      "\n",
      "LABEL DISTRIBUTION:\n",
      "Label\n",
      "0    40155\n",
      "1    11678\n",
      "Name: count, dtype: int64\n",
      "\n",
      "LABEL PERCENTAGE DISTRIBUTION:\n",
      "Label 0 (Normal/Benign): 77.47%\n",
      "Label 1 (Anomaly/Attack): 22.53%\n",
      "\n",
      "==================================================\n",
      "\n",
      "DETAILED BREAKDOWN:\n",
      "Rows with Label 0 (Normal): 40,155\n",
      "Rows with Label 1 (Anomaly): 11,678\n",
      "Total rows: 51,833\n",
      "\n",
      "==================================================\n",
      "\n",
      "DATASET SOURCES:\n",
      "From DOS files (mostly anomalies): 11,678 rows\n",
      "From Session files (normal traffic): 40,155 rows\n",
      "Combined total: 51,833 rows\n"
     ]
    }
   ],
   "source": [
    "# Concatenate both datasets\n",
    "df_combined = pd.concat([df_data, df_session_data], axis=0, ignore_index=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMBINED DATASET RESULTS:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"df_data (DOS files) shape: {df_data.shape}\")\n",
    "print(f\"df_session_data (Session files) shape: {df_session_data.shape}\")\n",
    "print(f\"Combined dataset shape: {df_combined.shape}\")\n",
    "\n",
    "print(f\"\\nTotal rows in combined dataset: {len(df_combined)}\")\n",
    "print(f\"Total columns in combined dataset: {len(df_combined.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Show label distribution\n",
    "print(\"LABEL DISTRIBUTION:\")\n",
    "label_counts = df_combined['Label'].value_counts().sort_index()\n",
    "print(label_counts)\n",
    "\n",
    "print(\"\\nLABEL PERCENTAGE DISTRIBUTION:\")\n",
    "label_percentages = df_combined['Label'].value_counts(normalize=True).sort_index() * 100\n",
    "print(f\"Label 0 (Normal/Benign): {label_percentages[0]:.2f}%\")\n",
    "print(f\"Label 1 (Anomaly/Attack): {label_percentages[1]:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"DETAILED BREAKDOWN:\")\n",
    "print(f\"Rows with Label 0 (Normal): {label_counts[0]:,}\")\n",
    "print(f\"Rows with Label 1 (Anomaly): {label_counts[1]:,}\")\n",
    "print(f\"Total rows: {label_counts.sum():,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"DATASET SOURCES:\")\n",
    "print(f\"From DOS files (mostly anomalies): {len(df_data):,} rows\")\n",
    "print(f\"From Session files (normal traffic): {len(df_session_data):,} rows\")\n",
    "print(f\"Combined total: {len(df_combined):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f7f091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f94d946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51833, 83)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf0561cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined=df_combined.drop(columns=['src_ip', 'dst_ip', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "247f4b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51833, 83)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe606b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.drop(columns=['src_ip', 'dst_ip', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab4d21a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "df['Label']= encoder.fit_transform(df['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f69b9812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.sum of Label\n",
       "0    40155\n",
       "1    11678\n",
       "Name: count, dtype: int64>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts().sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a061b26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs in DataFrame:\n",
      " src_port            0.0\n",
      "dst_port            0.0\n",
      "protocol            0.0\n",
      "flow_duration       0.0\n",
      "flow_byts_s         0.0\n",
      "                   ... \n",
      "subflow_fwd_pkts    0.0\n",
      "subflow_bwd_pkts    0.0\n",
      "subflow_fwd_byts    0.0\n",
      "subflow_bwd_byts    0.0\n",
      "Label               0.0\n",
      "Length: 80, dtype: float64\n",
      "Infs in DataFrame:\n",
      " src_port            0.0\n",
      "dst_port            0.0\n",
      "protocol            0.0\n",
      "flow_duration       0.0\n",
      "flow_byts_s         0.0\n",
      "                   ... \n",
      "subflow_fwd_pkts    0.0\n",
      "subflow_bwd_pkts    0.0\n",
      "subflow_fwd_byts    0.0\n",
      "subflow_bwd_byts    0.0\n",
      "Label               0.0\n",
      "Length: 80, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check for NaNs\n",
    "nan_mask = df.isna()\n",
    "print(\"NaNs in DataFrame:\\n\", df[nan_mask].sum())\n",
    "\n",
    "# Check for infinities\n",
    "inf_mask = df.isin([np.inf, -np.inf])\n",
    "print(\"Infs in DataFrame:\\n\", df[inf_mask].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef0b916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)  # Replace infinities with NaN\n",
    "df.fillna(0, inplace=True)  # Replace NaNs with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a376a05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "src_port            0\n",
       "dst_port            0\n",
       "protocol            0\n",
       "flow_duration       0\n",
       "flow_byts_s         0\n",
       "                   ..\n",
       "subflow_fwd_pkts    0\n",
       "subflow_bwd_pkts    0\n",
       "subflow_fwd_byts    0\n",
       "subflow_bwd_byts    0\n",
       "Label               0\n",
       "Length: 80, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f1ef4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_port</th>\n",
       "      <th>dst_port</th>\n",
       "      <th>protocol</th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>flow_byts_s</th>\n",
       "      <th>flow_pkts_s</th>\n",
       "      <th>fwd_pkts_s</th>\n",
       "      <th>bwd_pkts_s</th>\n",
       "      <th>tot_fwd_pkts</th>\n",
       "      <th>tot_bwd_pkts</th>\n",
       "      <th>...</th>\n",
       "      <th>fwd_blk_rate_avg</th>\n",
       "      <th>bwd_blk_rate_avg</th>\n",
       "      <th>fwd_seg_size_avg</th>\n",
       "      <th>bwd_seg_size_avg</th>\n",
       "      <th>cwr_flag_count</th>\n",
       "      <th>subflow_fwd_pkts</th>\n",
       "      <th>subflow_bwd_pkts</th>\n",
       "      <th>subflow_fwd_byts</th>\n",
       "      <th>subflow_bwd_byts</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59274</td>\n",
       "      <td>8009</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>218055</td>\n",
       "      <td>574</td>\n",
       "      <td>332</td>\n",
       "      <td>241</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>249</td>\n",
       "      <td>557</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>2748</td>\n",
       "      <td>4462</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59286</td>\n",
       "      <td>8009</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>225340</td>\n",
       "      <td>593</td>\n",
       "      <td>343</td>\n",
       "      <td>250</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>249</td>\n",
       "      <td>557</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>2748</td>\n",
       "      <td>4462</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63489</td>\n",
       "      <td>8009</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>249818</td>\n",
       "      <td>617</td>\n",
       "      <td>326</td>\n",
       "      <td>290</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>278</td>\n",
       "      <td>546</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>2508</td>\n",
       "      <td>4370</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63491</td>\n",
       "      <td>8009</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>370123</td>\n",
       "      <td>914</td>\n",
       "      <td>484</td>\n",
       "      <td>430</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>278</td>\n",
       "      <td>546</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>2508</td>\n",
       "      <td>4370</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59300</td>\n",
       "      <td>8009</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>218277</td>\n",
       "      <td>696</td>\n",
       "      <td>403</td>\n",
       "      <td>293</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>557</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>1495</td>\n",
       "      <td>4462</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51828</th>\n",
       "      <td>50557</td>\n",
       "      <td>20002</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>415</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>830</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51829</th>\n",
       "      <td>443</td>\n",
       "      <td>46848</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>563</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>2393</td>\n",
       "      <td>1151</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51830</th>\n",
       "      <td>53139</td>\n",
       "      <td>443</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>16088</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>393</td>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1575</td>\n",
       "      <td>423</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51831</th>\n",
       "      <td>58071</td>\n",
       "      <td>443</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>158621</td>\n",
       "      <td>192</td>\n",
       "      <td>70</td>\n",
       "      <td>122</td>\n",
       "      <td>336</td>\n",
       "      <td>585</td>\n",
       "      <td>...</td>\n",
       "      <td>120045</td>\n",
       "      <td>3028042</td>\n",
       "      <td>86</td>\n",
       "      <td>1247</td>\n",
       "      <td>0</td>\n",
       "      <td>336</td>\n",
       "      <td>585</td>\n",
       "      <td>29165</td>\n",
       "      <td>729942</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51832</th>\n",
       "      <td>443</td>\n",
       "      <td>50738</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51833 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       src_port  dst_port  protocol  flow_duration  flow_byts_s  flow_pkts_s  \\\n",
       "0         59274      8009         6              0       218055          574   \n",
       "1         59286      8009         6              0       225340          593   \n",
       "2         63489      8009         6              0       249818          617   \n",
       "3         63491      8009         6              0       370123          914   \n",
       "4         59300      8009         6              0       218277          696   \n",
       "...         ...       ...       ...            ...          ...          ...   \n",
       "51828     50557     20002        17              0            0            0   \n",
       "51829       443     46848        17              6          563            4   \n",
       "51830     53139       443        17              0        16088           64   \n",
       "51831     58071       443        17              4       158621          192   \n",
       "51832       443     50738         6              0            0            0   \n",
       "\n",
       "       fwd_pkts_s  bwd_pkts_s  tot_fwd_pkts  tot_bwd_pkts  ...  \\\n",
       "0             332         241            11             8  ...   \n",
       "1             343         250            11             8  ...   \n",
       "2             326         290             9             8  ...   \n",
       "3             484         430             9             8  ...   \n",
       "4             403         293            11             8  ...   \n",
       "...           ...         ...           ...           ...  ...   \n",
       "51828           0           0             2             0  ...   \n",
       "51829           2           1            17            11  ...   \n",
       "51830          32          32             4             4  ...   \n",
       "51831          70         122           336           585  ...   \n",
       "51832           0           0             2             0  ...   \n",
       "\n",
       "       fwd_blk_rate_avg  bwd_blk_rate_avg  fwd_seg_size_avg  bwd_seg_size_avg  \\\n",
       "0                     0                 0               249               557   \n",
       "1                     0                 0               249               557   \n",
       "2                     0                 0               278               546   \n",
       "3                     0                 0               278               546   \n",
       "4                     0                 0               135               557   \n",
       "...                 ...               ...               ...               ...   \n",
       "51828                 0                 0               415                 0   \n",
       "51829                 0                 0               140               104   \n",
       "51830                 0                 0               393               105   \n",
       "51831            120045           3028042                86              1247   \n",
       "51832                 0                 0                72                 0   \n",
       "\n",
       "       cwr_flag_count  subflow_fwd_pkts  subflow_bwd_pkts  subflow_fwd_byts  \\\n",
       "0                   0                11                 8              2748   \n",
       "1                   0                11                 8              2748   \n",
       "2                   0                 9                 8              2508   \n",
       "3                   0                 9                 8              2508   \n",
       "4                   0                11                 8              1495   \n",
       "...               ...               ...               ...               ...   \n",
       "51828               0                 2                 0               830   \n",
       "51829               0                17                11              2393   \n",
       "51830               0                 4                 4              1575   \n",
       "51831               0               336               585             29165   \n",
       "51832               0                 2                 0               144   \n",
       "\n",
       "       subflow_bwd_byts  Label  \n",
       "0                  4462      1  \n",
       "1                  4462      1  \n",
       "2                  4370      1  \n",
       "3                  4370      1  \n",
       "4                  4462      1  \n",
       "...                 ...    ...  \n",
       "51828                 0      0  \n",
       "51829              1151      0  \n",
       "51830               423      0  \n",
       "51831            729942      0  \n",
       "51832                 0      0  \n",
       "\n",
       "[51833 rows x 80 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.astype(int)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d2ad782",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Label',axis=1)\n",
    "y = df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f9a61c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       src_port  dst_port  protocol  flow_duration  flow_byts_s  flow_pkts_s  \\\n",
       " 0         59274      8009         6              0       218055          574   \n",
       " 1         59286      8009         6              0       225340          593   \n",
       " 2         63489      8009         6              0       249818          617   \n",
       " 3         63491      8009         6              0       370123          914   \n",
       " 4         59300      8009         6              0       218277          696   \n",
       " ...         ...       ...       ...            ...          ...          ...   \n",
       " 51828     50557     20002        17              0            0            0   \n",
       " 51829       443     46848        17              6          563            4   \n",
       " 51830     53139       443        17              0        16088           64   \n",
       " 51831     58071       443        17              4       158621          192   \n",
       " 51832       443     50738         6              0            0            0   \n",
       " \n",
       "        fwd_pkts_s  bwd_pkts_s  tot_fwd_pkts  tot_bwd_pkts  ...  \\\n",
       " 0             332         241            11             8  ...   \n",
       " 1             343         250            11             8  ...   \n",
       " 2             326         290             9             8  ...   \n",
       " 3             484         430             9             8  ...   \n",
       " 4             403         293            11             8  ...   \n",
       " ...           ...         ...           ...           ...  ...   \n",
       " 51828           0           0             2             0  ...   \n",
       " 51829           2           1            17            11  ...   \n",
       " 51830          32          32             4             4  ...   \n",
       " 51831          70         122           336           585  ...   \n",
       " 51832           0           0             2             0  ...   \n",
       " \n",
       "        bwd_pkts_b_avg  fwd_blk_rate_avg  bwd_blk_rate_avg  fwd_seg_size_avg  \\\n",
       " 0                   0                 0                 0               249   \n",
       " 1                   0                 0                 0               249   \n",
       " 2                   0                 0                 0               278   \n",
       " 3                   0                 0                 0               278   \n",
       " 4                   0                 0                 0               135   \n",
       " ...               ...               ...               ...               ...   \n",
       " 51828               0                 0                 0               415   \n",
       " 51829               0                 0                 0               140   \n",
       " 51830               0                 0                 0               393   \n",
       " 51831              12            120045           3028042                86   \n",
       " 51832               0                 0                 0                72   \n",
       " \n",
       "        bwd_seg_size_avg  cwr_flag_count  subflow_fwd_pkts  subflow_bwd_pkts  \\\n",
       " 0                   557               0                11                 8   \n",
       " 1                   557               0                11                 8   \n",
       " 2                   546               0                 9                 8   \n",
       " 3                   546               0                 9                 8   \n",
       " 4                   557               0                11                 8   \n",
       " ...                 ...             ...               ...               ...   \n",
       " 51828                 0               0                 2                 0   \n",
       " 51829               104               0                17                11   \n",
       " 51830               105               0                 4                 4   \n",
       " 51831              1247               0               336               585   \n",
       " 51832                 0               0                 2                 0   \n",
       " \n",
       "        subflow_fwd_byts  subflow_bwd_byts  \n",
       " 0                  2748              4462  \n",
       " 1                  2748              4462  \n",
       " 2                  2508              4370  \n",
       " 3                  2508              4370  \n",
       " 4                  1495              4462  \n",
       " ...                 ...               ...  \n",
       " 51828               830                 0  \n",
       " 51829              2393              1151  \n",
       " 51830              1575               423  \n",
       " 51831             29165            729942  \n",
       " 51832               144                 0  \n",
       " \n",
       " [51833 rows x 79 columns],\n",
       " 0        1\n",
       " 1        1\n",
       " 2        1\n",
       " 3        1\n",
       " 4        1\n",
       "         ..\n",
       " 51828    0\n",
       " 51829    0\n",
       " 51830    0\n",
       " 51831    0\n",
       " 51832    0\n",
       " Name: Label, Length: 51833, dtype: int32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c30498aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fbd8149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "# Impute missing values (replace NaNs with the mean)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Determine the number of columns (features) in your DataFrame\n",
    "num_columns = df.shape[1]\n",
    "\n",
    "# Set an appropriate value for k (less than or equal to the number of columns)\n",
    "k = min(10, num_columns)  # Adjust this as needed\n",
    "\n",
    "# Initialize SelectKBest with the scoring function\n",
    "k_best = SelectKBest(score_func=f_classif, k=k)\n",
    "\n",
    "# Fit and transform the imputed data to select the top 10 features\n",
    "X_new = k_best.fit_transform(X_imputed, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e14b722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False,  True, False, False, False,\n",
       "       False, False, False, False,  True,  True, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the boolean mask of selected features\n",
    "selected_features_mask = k_best.get_support()\n",
    "selected_features_mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ddf528cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "elected_feature_names = X.columns[selected_features_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9ae8138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['src_port', 'protocol', 'fwd_pkt_len_max', 'fwd_pkt_len_mean',\n",
       "       'pkt_len_mean', 'pkt_len_std', 'fwd_iat_tot', 'syn_flag_cnt',\n",
       "       'pkt_size_avg', 'fwd_seg_size_avg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elected_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5cd169d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns=['dst_port','flow_duration', 'fwd_pkt_len_max', 'fwd_pkt_len_mean',\n",
    "       'pkt_len_mean', 'pkt_len_std', 'fwd_iat_tot', 'syn_flag_cnt',\n",
    "       'pkt_size_avg', 'fwd_seg_size_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f532ac8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dst_port</th>\n",
       "      <th>flow_duration</th>\n",
       "      <th>fwd_pkt_len_max</th>\n",
       "      <th>fwd_pkt_len_mean</th>\n",
       "      <th>pkt_len_mean</th>\n",
       "      <th>pkt_len_std</th>\n",
       "      <th>fwd_iat_tot</th>\n",
       "      <th>syn_flag_cnt</th>\n",
       "      <th>pkt_size_avg</th>\n",
       "      <th>fwd_seg_size_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8009</td>\n",
       "      <td>0</td>\n",
       "      <td>1842</td>\n",
       "      <td>249</td>\n",
       "      <td>379</td>\n",
       "      <td>577</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>379</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8009</td>\n",
       "      <td>0</td>\n",
       "      <td>1842</td>\n",
       "      <td>249</td>\n",
       "      <td>379</td>\n",
       "      <td>577</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>379</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8009</td>\n",
       "      <td>0</td>\n",
       "      <td>1798</td>\n",
       "      <td>278</td>\n",
       "      <td>404</td>\n",
       "      <td>595</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>404</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8009</td>\n",
       "      <td>0</td>\n",
       "      <td>1798</td>\n",
       "      <td>278</td>\n",
       "      <td>404</td>\n",
       "      <td>595</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>404</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8009</td>\n",
       "      <td>0</td>\n",
       "      <td>589</td>\n",
       "      <td>135</td>\n",
       "      <td>313</td>\n",
       "      <td>468</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>313</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51828</th>\n",
       "      <td>20002</td>\n",
       "      <td>0</td>\n",
       "      <td>415</td>\n",
       "      <td>415</td>\n",
       "      <td>415</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>415</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51829</th>\n",
       "      <td>46848</td>\n",
       "      <td>6</td>\n",
       "      <td>278</td>\n",
       "      <td>140</td>\n",
       "      <td>126</td>\n",
       "      <td>66</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51830</th>\n",
       "      <td>443</td>\n",
       "      <td>0</td>\n",
       "      <td>706</td>\n",
       "      <td>393</td>\n",
       "      <td>249</td>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>249</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51831</th>\n",
       "      <td>443</td>\n",
       "      <td>4</td>\n",
       "      <td>191</td>\n",
       "      <td>86</td>\n",
       "      <td>824</td>\n",
       "      <td>587</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>824</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51832</th>\n",
       "      <td>50738</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51833 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dst_port  flow_duration  fwd_pkt_len_max  fwd_pkt_len_mean  \\\n",
       "0          8009              0             1842               249   \n",
       "1          8009              0             1842               249   \n",
       "2          8009              0             1798               278   \n",
       "3          8009              0             1798               278   \n",
       "4          8009              0              589               135   \n",
       "...         ...            ...              ...               ...   \n",
       "51828     20002              0              415               415   \n",
       "51829     46848              6              278               140   \n",
       "51830       443              0              706               393   \n",
       "51831       443              4              191                86   \n",
       "51832     50738              0               72                72   \n",
       "\n",
       "       pkt_len_mean  pkt_len_std  fwd_iat_tot  syn_flag_cnt  pkt_size_avg  \\\n",
       "0               379          577            0             3           379   \n",
       "1               379          577            0             3           379   \n",
       "2               404          595            0             3           404   \n",
       "3               404          595            0             3           404   \n",
       "4               313          468            0             3           313   \n",
       "...             ...          ...          ...           ...           ...   \n",
       "51828           415            0            0             0           415   \n",
       "51829           126           66            6             0           126   \n",
       "51830           249          266            0             0           249   \n",
       "51831           824          587            4             0           824   \n",
       "51832            72            0            0             0            72   \n",
       "\n",
       "       fwd_seg_size_avg  \n",
       "0                   249  \n",
       "1                   249  \n",
       "2                   278  \n",
       "3                   278  \n",
       "4                   135  \n",
       "...                 ...  \n",
       "51828               415  \n",
       "51829               140  \n",
       "51830               393  \n",
       "51831                86  \n",
       "51832                72  \n",
       "\n",
       "[51833 rows x 10 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new=X[new_columns]\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7202b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        1\n",
       "2        1\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "51828    0\n",
       "51829    0\n",
       "51830    0\n",
       "51831    0\n",
       "51832    0\n",
       "Name: label, Length: 51833, dtype: int32"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new['label']=df['Label']\n",
    "df_new['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a7a31ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = df_new[new_columns].values  # Get the 10 selected features\n",
    "y1 = df_new['label'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a81b1148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 8009,     0,  1842, ...,     3,   379,   249],\n",
       "        [ 8009,     0,  1842, ...,     3,   379,   249],\n",
       "        [ 8009,     0,  1798, ...,     3,   404,   278],\n",
       "        ...,\n",
       "        [  443,     0,   706, ...,     0,   249,   393],\n",
       "        [  443,     4,   191, ...,     0,   824,    86],\n",
       "        [50738,     0,    72, ...,     0,    72,    72]]),\n",
       " array([1, 1, 1, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1, y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8cea8e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51833, 10)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01540e19",
   "metadata": {},
   "source": [
    "Modelling Stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aebd940",
   "metadata": {},
   "source": [
    "SafeML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72571c68",
   "metadata": {},
   "source": [
    "Running the Statistical distance measure algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b63fc46f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Anderson_Darling_dist</th>\n",
       "      <th>CVM_dist</th>\n",
       "      <th>DTS_dist</th>\n",
       "      <th>Kolmogorov_Smirnov_dist</th>\n",
       "      <th>Kuiper_dist</th>\n",
       "      <th>Wasserstein_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Accuracy, Anderson_Darling_dist, CVM_dist, DTS_dist, Kolmogorov_Smirnov_dist, Kuiper_dist, Wasserstein_dist]\n",
       "Index: []"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Firstly, creating the final 2D-array (Pandas Dataframe) which will be used to store the Results\n",
    "# PRESERVE THE ORDERING\n",
    "results_column_names = ['Accuracy', 'Anderson_Darling_dist', 'CVM_dist',\n",
    "                                     'DTS_dist', 'Kolmogorov_Smirnov_dist','Kuiper_dist', 'Wasserstein_dist']\n",
    "# Creating the empty Dataframe for Results\n",
    "df_results = pd.DataFrame(columns = results_column_names)\n",
    "# Can copy this dataframe for future results tables e.g. for each class/ label\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dd8e03",
   "metadata": {},
   "source": [
    "Code for each permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af5c5a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting single DNN training...\n",
      "\n",
      "✅ Training completed! Stopped at epoch 13\n",
      "✅ Model trained. Accuracy: 0.9980\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set up variables\n",
    "labels = df['Label'].unique()\n",
    "number_of_classes = len(labels)\n",
    "list_of_lists_results = [[] for _ in range(number_of_classes)]\n",
    "\n",
    "print(\"🚀 Starting single DNN training...\")\n",
    "\n",
    "# 1. Split data (stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X1, y1, test_size=0.33, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 2. Train DNN model and get predictions\n",
    "pred_y, accuracy, trained_model, scaler = train_and_predict_DNN_model(X_train, X_test, y_train, y_test)\n",
    "print(f\"✅ Model trained. Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad04ee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test samples: 17105\n",
      "Predictions breakdown:\n",
      "   Class 0 (Normal): 13,273 samples (77.6%)\n",
      "   Class 1 (Anomaly): 3,832 samples (22.4%)\n"
     ]
    }
   ],
   "source": [
    "# Count predictions for each class\n",
    "import numpy as np\n",
    "pred_counts = np.bincount(pred_y)\n",
    "total_predictions = len(pred_y)\n",
    "\n",
    "print(f\"Total test samples: {total_predictions}\")\n",
    "print(f\"Predictions breakdown:\")\n",
    "for class_idx, count in enumerate(pred_counts):\n",
    "    percentage = (count / total_predictions) * 100\n",
    "    class_name = \"Normal\" if class_idx == 0 else \"Anomaly\"\n",
    "    print(f\"   Class {class_idx} ({class_name}): {count:,} samples ({percentage:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8657c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED: Sort labels to ensure [0, 1] order\n",
    "labels = sorted(df['Label'].unique())  # This ensures [0, 1] instead of [1, 0]\n",
    "number_of_classes = len(labels)\n",
    "list_of_lists_results = [[] for _ in range(number_of_classes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8826cf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training data shape for Class 0: (26904, 10)\n",
      "   Test data shape for Class 0: (13273, 10)\n",
      "   Training samples: 26904 rows\n",
      "   Test samples: 13273 rows\n",
      "   Features: 10\n",
      "👉 After sampling: Class 0 test set has 13273 samples (target: 13273)\n",
      "   Training data shape for Class 1: (7824, 10)\n",
      "   Test data shape for Class 1: (3832, 10)\n",
      "   Training samples: 7824 rows\n",
      "   Test samples: 3832 rows\n",
      "   Features: 10\n",
      "👉 After sampling: Class 1 test set has 3832 samples (target: 3832)\n",
      "\n",
      "============================================================\n",
      "🏆 MODEL SAVED!\n",
      "============================================================\n",
      "✅ Model saved as: best_dnn_model_final.h5\n",
      "✅ Scaler saved as: best_scaler_final.pkl\n",
      "📊 Accuracy achieved: 0.9980 (99.80%)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "desired_samples = {0: 13273, 1: 3832}  \n",
    "# 3. Loop over each label/class for ECDF statistical distance measures\n",
    "for current_label in range(number_of_classes):\n",
    "    X_train_loc_for_label, X_test_loc_for_label = get_X_train_and_test_data_for_given_label(\n",
    "        labels, current_label, pred_y, X_train, X_test, y_train, y_test\n",
    "    )\n",
    "    print(f\"   Training data shape for Class {current_label}: {X_train_loc_for_label.shape}\")\n",
    "    print(f\"   Test data shape for Class {current_label}: {X_test_loc_for_label.shape}\")\n",
    "    print(f\"   Training samples: {len(X_train_loc_for_label)} rows\")\n",
    "    print(f\"   Test samples: {len(X_test_loc_for_label)} rows\")\n",
    "    print(f\"   Features: {X_train_loc_for_label.shape[1] if len(X_train_loc_for_label) > 0 else 'N/A'}\")\n",
    "\n",
    "    # --- FORCE TEST DATA TO DESIRED SIZE ---\n",
    "    n_desired = desired_samples.get(current_label, len(X_test_loc_for_label))\n",
    "    if len(X_test_loc_for_label) > n_desired:\n",
    "        X_test_loc_for_label = X_test_loc_for_label.sample(n=n_desired, random_state=42)\n",
    "    elif len(X_test_loc_for_label) < n_desired:\n",
    "        print(f\"⚠️ Not enough samples for Class {current_label}: requested {n_desired}, available {len(X_test_loc_for_label)}. Oversampling with replacement.\")\n",
    "        X_test_loc_for_label = X_test_loc_for_label.sample(n=n_desired, replace=True, random_state=42)\n",
    "\n",
    "    # Show the number of samples after forcing the size\n",
    "    print(f\"👉 After sampling: Class {current_label} test set has {len(X_test_loc_for_label)} samples (target: {n_desired})\")\n",
    "\n",
    "    dict_result_row = get_statistical_dist_measures_for_class_result(\n",
    "        accuracy, X_train_loc_for_label, X_test_loc_for_label\n",
    "    )\n",
    "    list_of_lists_results[current_label].append(dict_result_row)\n",
    "\n",
    "# 4. Save the trained model and scaler\n",
    "trained_model.save(\"best_dnn_model_final.h5\")\n",
    "with open(\"best_scaler_final.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🏆 MODEL SAVED!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"✅ Model saved as: best_dnn_model_final.h5\")\n",
    "print(f\"✅ Scaler saved as: best_scaler_final.pkl\")\n",
    "print(f\"📊 Accuracy achieved: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60c489c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access specific dataframe by index e.g. class 1 dataframe->index 1 mapping\n",
    "result_dataframes = []\n",
    "\n",
    "for dict_result_list in list_of_lists_results:\n",
    "    result_dataframes.append(pd.DataFrame(dict_result_list, columns = results_column_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "21d1ba00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Anderson_Darling_dist</th>\n",
       "      <th>CVM_dist</th>\n",
       "      <th>DTS_dist</th>\n",
       "      <th>Kolmogorov_Smirnov_dist</th>\n",
       "      <th>Kuiper_dist</th>\n",
       "      <th>Wasserstein_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.998012</td>\n",
       "      <td>0.025108</td>\n",
       "      <td>1.569996</td>\n",
       "      <td>0.284515</td>\n",
       "      <td>0.007785</td>\n",
       "      <td>0.010759</td>\n",
       "      <td>10.844416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Anderson_Darling_dist  CVM_dist  DTS_dist  \\\n",
       "0  0.998012               0.025108  1.569996  0.284515   \n",
       "\n",
       "   Kolmogorov_Smirnov_dist  Kuiper_dist  Wasserstein_dist  \n",
       "0                 0.007785     0.010759         10.844416  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print first dataframe result table, for class 0\n",
    "result_dataframes[0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c66fa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframes[0].to_excel(\"Class0.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "27d1d779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Anderson_Darling_dist</th>\n",
       "      <th>CVM_dist</th>\n",
       "      <th>DTS_dist</th>\n",
       "      <th>Kolmogorov_Smirnov_dist</th>\n",
       "      <th>Kuiper_dist</th>\n",
       "      <th>Wasserstein_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.998012</td>\n",
       "      <td>0.006444</td>\n",
       "      <td>0.119378</td>\n",
       "      <td>0.294817</td>\n",
       "      <td>0.011688</td>\n",
       "      <td>0.011707</td>\n",
       "      <td>3.220949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy  Anderson_Darling_dist  CVM_dist  DTS_dist  \\\n",
       "0  0.998012               0.006444  0.119378  0.294817   \n",
       "\n",
       "   Kolmogorov_Smirnov_dist  Kuiper_dist  Wasserstein_dist  \n",
       "0                 0.011688     0.011707          3.220949  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print second dataframe result table, for class 1\n",
    "result_dataframes[1].head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5557bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframes[1].to_excel(\"Class1.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e66e162",
   "metadata": {},
   "source": [
    "Plotting the SafeML Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a7bcd844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading real-time data from real.csv ...\n",
      "✅ Loaded 25916 samples from real.csv\n",
      "   Original shape: (25916, 83)\n",
      "\n",
      "🎯 Step 2: Selecting the same 10 features as training...\n",
      "✅ Selected same 10 features: (25916, 10)\n",
      "\n",
      "🔧 Step 3: Applying same preprocessing as training...\n",
      "   Handling NaNs and infinities...\n",
      "   Converting to integer type...\n",
      "✅ Preprocessed data shape: (25916, 10)\n",
      "\n",
      "🤖 Step 4: Loading pretrained scaler and applying scaling...\n",
      "✅ Applied saved scaler to real-time data\n",
      "   Scaled data shape: (25916, 10)\n",
      "\n",
      "🔮 Step 5: Loading pretrained model and making predictions...\n",
      "✅ Loaded trained model from best_dnn_model_final.h5\n",
      "   Model input shape: (None, 10)\n",
      "   Real-time data shape: (25916, 10)\n",
      "✅ Predictions completed:\n",
      "   Normal traffic (Class 0): 20089 samples (77.5%)\n",
      "   Anomaly traffic (Class 1): 5827 samples (22.5%)\n"
     ]
    }
   ],
   "source": [
    "def predict_realtime_with_pretrained_model(\n",
    "    data_path=\"real.csv\",\n",
    "    model_path=\"best_dnn_model_final.h5\",\n",
    "    scaler_path=\"best_scaler_final.pkl\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Predicts using a pretrained DNN model and scaler on real-time data.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the real-time CSV data.\n",
    "        model_path (str): Path to the pretrained Keras model (.h5).\n",
    "        scaler_path (str): Path to the pretrained scaler (.pkl).\n",
    "\n",
    "    Returns:\n",
    "        dict: Prediction results and statistics.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "    import os\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    print(f\"📥 Loading real-time data from {data_path} ...\")\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"❌ Data file not found: {data_path}\")\n",
    "        return None\n",
    "\n",
    "    df_realtime = pd.read_csv(data_path)\n",
    "    print(f\"✅ Loaded {len(df_realtime)} samples from {data_path}\")\n",
    "    print(f\"   Original shape: {df_realtime.shape}\")\n",
    "\n",
    "    # Save true labels if available for comparison\n",
    "    y_true = None\n",
    "    if ' Label' in df_realtime.columns:\n",
    "        encoder = LabelEncoder()\n",
    "        y_true = encoder.fit_transform(df_realtime[' Label'])\n",
    "        df_realtime = df_realtime.drop(' Label', axis=1)\n",
    "        print(\"   True labels saved for comparison\")\n",
    "\n",
    "    # Step 2: Select ONLY the new_columns (same 10 features as training)\n",
    "    print(\"\\n🎯 Step 2: Selecting the same 10 features as training...\")\n",
    "    new_columns = ['dst_port','flow_duration', 'fwd_pkt_len_max', 'fwd_pkt_len_mean',\n",
    "       'pkt_len_mean', 'pkt_len_std', 'fwd_iat_tot', 'syn_flag_cnt',\n",
    "       'pkt_size_avg', 'fwd_seg_size_avg']\n",
    "    missing_cols = [col for col in new_columns if col not in df_realtime.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"❌ Missing required columns: {missing_cols}\")\n",
    "        return None\n",
    "\n",
    "    df_realtime_selected = df_realtime[new_columns].copy()\n",
    "    print(f\"✅ Selected same 10 features: {df_realtime_selected.shape}\")\n",
    "\n",
    "    # Step 3: Apply same preprocessing as training\n",
    "    print(\"\\n🔧 Step 3: Applying same preprocessing as training...\")\n",
    "    print(\"   Handling NaNs and infinities...\")\n",
    "    df_realtime_selected.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_realtime_selected.fillna(0, inplace=True)\n",
    "    print(\"   Converting to integer type...\")\n",
    "    df_realtime_selected = df_realtime_selected.astype(int)\n",
    "    print(f\"✅ Preprocessed data shape: {df_realtime_selected.shape}\")\n",
    "\n",
    "    # Step 4: Load pretrained scaler and apply scaling\n",
    "    print(\"\\n🤖 Step 4: Loading pretrained scaler and applying scaling...\")\n",
    "    if not os.path.exists(scaler_path):\n",
    "        print(f\"❌ Scaler file not found: {scaler_path}\")\n",
    "        return None\n",
    "    with open(scaler_path, \"rb\") as f:\n",
    "        scaler_loaded = pickle.load(f)\n",
    "    X_realtime_scaled = scaler_loaded.transform(df_realtime_selected)\n",
    "    print(f\"✅ Applied saved scaler to real-time data\")\n",
    "    print(f\"   Scaled data shape: {X_realtime_scaled.shape}\")\n",
    "\n",
    "    # Step 5: Load pretrained model and make predictions\n",
    "    print(\"\\n🔮 Step 5: Loading pretrained model and making predictions...\")\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"❌ Model file not found: {model_path}\")\n",
    "        return None\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(f\"✅ Loaded trained model from {model_path}\")\n",
    "    print(f\"   Model input shape: {model.input_shape}\")\n",
    "    print(f\"   Real-time data shape: {X_realtime_scaled.shape}\")\n",
    "\n",
    "    pred_prob = model.predict(X_realtime_scaled, verbose=0)\n",
    "    pred_labels = (pred_prob > 0.5).astype(int).flatten()\n",
    "\n",
    "    normal_count = np.sum(pred_labels == 0)\n",
    "    anomaly_count = np.sum(pred_labels == 1)\n",
    "    print(f\"✅ Predictions completed:\")\n",
    "    print(f\"   Normal traffic (Class 0): {normal_count} samples ({normal_count/len(pred_labels)*100:.1f}%)\")\n",
    "    print(f\"   Anomaly traffic (Class 1): {anomaly_count} samples ({anomaly_count/len(pred_labels)*100:.1f}%)\")\n",
    "\n",
    "    results = {\n",
    "        \"total_samples\": len(df_realtime_selected),\n",
    "        \"normal_count\": normal_count,\n",
    "        \"anomaly_count\": anomaly_count,\n",
    "        \"normal_percentage\": normal_count / len(pred_labels) * 100,\n",
    "        \"anomaly_percentage\": anomaly_count / len(pred_labels) * 100,\n",
    "        \"predictions\": pred_labels,\n",
    "        \"prediction_probabilities\": pred_prob.flatten(),\n",
    "        \"y_true\": y_true\n",
    "    }\n",
    "    return results\n",
    "\n",
    "#Example usage:\n",
    "results = predict_realtime_with_pretrained_model(\n",
    "    data_path=\"real.csv\",\n",
    "    model_path=\"best_dnn_model_final.h5\",\n",
    "    scaler_path=\"best_scaler_final.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "da9236ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading real-time data from real.csv ...\n",
      "✅ Loaded 25916 samples from real.csv\n",
      "   Original shape: (25916, 83)\n",
      "\n",
      "🎯 Step 2: Selecting the same 10 features as training...\n",
      "✅ Selected same 10 features: (25916, 10)\n",
      "\n",
      "🔧 Step 3: Applying same preprocessing as training...\n",
      "   Handling NaNs and infinities...\n",
      "   Converting to integer type...\n",
      "✅ Preprocessed data shape: (25916, 10)\n",
      "\n",
      "🤖 Step 4: Loading pretrained scaler and applying scaling...\n",
      "✅ Applied saved scaler to real-time data\n",
      "   Scaled data shape: (25916, 10)\n",
      "\n",
      "🔮 Step 5: Loading pretrained model and making predictions...\n",
      "✅ Loaded trained model from best_dnn_model_final.h5\n",
      "   Model input shape: (None, 10)\n",
      "   Real-time data shape: (25916, 10)\n",
      "✅ Predictions completed:\n",
      "   Normal traffic (Class 0): 20089 samples (77.5%)\n",
      "   Anomaly traffic (Class 1): 5827 samples (22.5%)\n",
      "Class 0: Train samples = 26904, Test samples = 13273\n",
      "Class 0 metrics: {'Accuracy': 1.0, 'Anderson_Darling_dist': 0.012788796898042557, 'CVM_dist': 0.6782712861767471, 'DTS_dist': 0.2257839346985151, 'Kolmogorov_Smirnov_dist': 0.0037453918096421513, 'Kuiper_dist': 0.005282219673447783, 'Wasserstein_dist': 10.033164114628043}\n",
      "Class 1: Train samples = 7824, Test samples = 3832\n",
      "Class 1 metrics: {'Accuracy': 1.0, 'Anderson_Darling_dist': 0.005084584813652106, 'CVM_dist': 0.07736430596361035, 'DTS_dist': 0.2762892130431863, 'Kolmogorov_Smirnov_dist': 0.0067172737596940205, 'Kuiper_dist': 0.007308570385670199, 'Wasserstein_dist': 2.4196904978574567}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Run your prediction function and get results\n",
    "results = predict_realtime_with_pretrained_model(\n",
    "    data_path=\"real.csv\",\n",
    "    model_path=\"best_dnn_model_final.h5\",\n",
    "    scaler_path=\"best_scaler_final.pkl\"\n",
    ")\n",
    "\n",
    "# 2. Prepare the DataFrame with the same 10 features as used in prediction\n",
    "df_realtime = pd.read_csv(\"real.csv\")\n",
    "new_columns = ['dst_port','flow_duration', 'fwd_pkt_len_max', 'fwd_pkt_len_mean',\n",
    "       'pkt_len_mean', 'pkt_len_std', 'fwd_iat_tot', 'syn_flag_cnt',\n",
    "       'pkt_size_avg', 'fwd_seg_size_avg']\n",
    "df_realtime_selected = df_realtime[new_columns].copy()\n",
    "df_realtime_selected.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_realtime_selected.fillna(0, inplace=True)\n",
    "df_realtime_selected = df_realtime_selected.astype(int)\n",
    "\n",
    "# 3. Get predicted labels\n",
    "pred_labels = results['predictions']\n",
    "\n",
    "# 4. Select indices for each class\n",
    "idx_class0 = np.where(pred_labels == 0)[0]\n",
    "idx_class1 = np.where(pred_labels == 1)[0]\n",
    "\n",
    "# 5. Sample the required number of samples for each class\n",
    "desired_samples = {0: 13273, 1: 3832} \n",
    "np.random.seed(42)\n",
    "if len(idx_class0) >= desired_samples[0]:\n",
    "    idx_class0_sampled = np.random.choice(idx_class0, desired_samples[0], replace=False)\n",
    "else:\n",
    "    idx_class0_sampled = np.random.choice(idx_class0, desired_samples[0], replace=True)\n",
    "if len(idx_class1) >= desired_samples[1]:\n",
    "    idx_class1_sampled = np.random.choice(idx_class1, desired_samples[1], replace=False)\n",
    "else:\n",
    "    idx_class1_sampled = np.random.choice(idx_class1, desired_samples[1], replace=True)\n",
    "\n",
    "# 6. Concatenate the sampled indices\n",
    "idx_final = np.concatenate([idx_class0_sampled, idx_class1_sampled])\n",
    "X_test_for_metrics = df_realtime_selected.iloc[idx_final].reset_index(drop=True)\n",
    "\n",
    "# 7. For the training data, use your existing X_train (should be the same 10 features, already preprocessed)\n",
    "#    If X_train is not a DataFrame, convert it:\n",
    "if not isinstance(X_train, pd.DataFrame):\n",
    "    X_train_df = pd.DataFrame(X_train, columns=new_columns)\n",
    "else:\n",
    "    X_train_df = X_train\n",
    "\n",
    "# 8. Calculate metrics for each class using your function\n",
    "metrics_results = []\n",
    "for label, n_samples in desired_samples.items():\n",
    "    # Training data for this class\n",
    "    X_train_L = X_train_df[y_train == label]\n",
    "    # Test data for this class\n",
    "    X_test_L = X_test_for_metrics.iloc[\n",
    "        np.where(pred_labels[idx_final] == label)[0]\n",
    "    ]\n",
    "    # Print number of samples in train and test for this class\n",
    "    print(f\"Class {label}: Train samples = {len(X_train_L)}, Test samples = {len(X_test_L)}\")\n",
    "    # Calculate metrics (set accuracy=1)\n",
    "    metrics = get_statistical_dist_measures_for_class_result(\n",
    "        accuracy=1.0, X_train_L=X_train_L, X_test_L=X_test_L\n",
    "    )\n",
    "    print(f\"Class {label} metrics:\", metrics)\n",
    "    metrics_results.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cb5ee70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert metrics_results to the required dictionaries for each class\n",
    "realtime_distances_class0 = {\n",
    "    'Anderson_Darling_dist': metrics_results[0]['Anderson_Darling_dist'],\n",
    "    'CVM_dist': metrics_results[0]['CVM_dist'],\n",
    "    'DTS_dist': metrics_results[0]['DTS_dist'],\n",
    "    'Kolmogorov_Smirnov_dist': metrics_results[0]['Kolmogorov_Smirnov_dist'],\n",
    "    'Kuiper_dist': metrics_results[0]['Kuiper_dist'],\n",
    "    'Wasserstein_dist': metrics_results[0]['Wasserstein_dist']\n",
    "}\n",
    "realtime_distances_class1 = {\n",
    "    'Anderson_Darling_dist': metrics_results[1]['Anderson_Darling_dist'],\n",
    "    'CVM_dist': metrics_results[1]['CVM_dist'],\n",
    "    'DTS_dist': metrics_results[1]['DTS_dist'],\n",
    "    'Kolmogorov_Smirnov_dist': metrics_results[1]['Kolmogorov_Smirnov_dist'],\n",
    "    'Kuiper_dist': metrics_results[1]['Kuiper_dist'],\n",
    "    'Wasserstein_dist': metrics_results[1]['Wasserstein_dist']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2d2ecc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Class 0:\n",
      "Anderson_Darling_dist: training = 0.025108 | realtime = 0.012789 | ratio = 0.51\n",
      "CVM_dist: training = 1.569996 | realtime = 0.678271 | ratio = 0.43\n",
      "DTS_dist: training = 0.284515 | realtime = 0.225784 | ratio = 0.79\n",
      "Kolmogorov_Smirnov_dist: training = 0.007785 | realtime = 0.003745 | ratio = 0.48\n",
      "Kuiper_dist: training = 0.010759 | realtime = 0.005282 | ratio = 0.49\n",
      "Wasserstein_dist: training = 10.844416 | realtime = 10.033164 | ratio = 0.93\n",
      "\n",
      "Class 1:\n",
      "Anderson_Darling_dist: training = 0.006444 | realtime = 0.005085 | ratio = 0.79\n",
      "CVM_dist: training = 0.119378 | realtime = 0.077364 | ratio = 0.65\n",
      "DTS_dist: training = 0.294817 | realtime = 0.276289 | ratio = 0.94\n",
      "Kolmogorov_Smirnov_dist: training = 0.011688 | realtime = 0.006717 | ratio = 0.57\n",
      "Kuiper_dist: training = 0.011707 | realtime = 0.007309 | ratio = 0.62\n",
      "Wasserstein_dist: training = 3.220949 | realtime = 2.419690 | ratio = 0.75\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load training results\n",
    "df_train_class0 = pd.read_excel(\"Class0.xlsx\")\n",
    "df_train_class1 = pd.read_excel(\"Class1.xlsx\")\n",
    "\n",
    "# Get all training distances (as arrays)\n",
    "train0 = df_train_class0.iloc[0]  # If only one row, or use .mean() for average\n",
    "train1 = df_train_class1.iloc[0]\n",
    "\n",
    "# Get real-time results (from your previous metrics_results)\n",
    "realtime0 = metrics_results[0]\n",
    "realtime1 = metrics_results[1]\n",
    "\n",
    "distance_names = [\n",
    "    'Anderson_Darling_dist', 'CVM_dist', 'DTS_dist',\n",
    "    'Kolmogorov_Smirnov_dist', 'Kuiper_dist', 'Wasserstein_dist'\n",
    "]\n",
    "\n",
    "print(\"\\nClass 0:\")\n",
    "for d in distance_names:\n",
    "    train_val = train0[d]\n",
    "    real_val = realtime0[d]\n",
    "    ratio = real_val / train_val if train_val != 0 else float('inf')\n",
    "    print(f\"{d}: training = {train_val:.6f} | realtime = {real_val:.6f} | ratio = {ratio:.2f}\")\n",
    "\n",
    "print(\"\\nClass 1:\")\n",
    "for d in distance_names:\n",
    "    train_val = train1[d]\n",
    "    real_val = realtime1[d]\n",
    "    ratio = real_val / train_val if train_val != 0 else float('inf')\n",
    "    print(f\"{d}: training = {train_val:.6f} | realtime = {real_val:.6f} | ratio = {ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c24d8b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_accuracy_from_distance_ratios(training_data_path_class0, training_data_path_class1, \n",
    "                                         realtime_distances_class0, realtime_distances_class1):\n",
    "    \"\"\"\n",
    "    Estimate accuracy based on distance ratios between training and real-time data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    training_data_path_class0 : str\n",
    "        Path to Class0.xlsx file\n",
    "    training_data_path_class1 : str  \n",
    "        Path to Class1.xlsx file\n",
    "    realtime_distances_class0 : dict\n",
    "        Real-time distances for class 0\n",
    "    realtime_distances_class1 : dict\n",
    "        Real-time distances for class 1\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (estimated_accuracy, detailed_analysis)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"📊 Loading training data and calculating accuracy estimation...\")\n",
    "        \n",
    "        # Load training data\n",
    "        df_class0 = pd.read_excel(training_data_path_class0)\n",
    "        df_class1 = pd.read_excel(training_data_path_class1)\n",
    "        \n",
    "        # Get best accuracy from each class (assuming first row or max accuracy)\n",
    "        if 'Accuracy' in df_class0.columns:\n",
    "            best_accuracy_class0 = df_class0['Accuracy'].max()\n",
    "            best_idx_class0 = df_class0['Accuracy'].idxmax()\n",
    "        else:\n",
    "            best_accuracy_class0 = df_class0.iloc[0, 0]  # Assume first column is accuracy\n",
    "            best_idx_class0 = 0\n",
    "            \n",
    "        if 'Accuracy' in df_class1.columns:\n",
    "            best_accuracy_class1 = df_class1['Accuracy'].max()\n",
    "            best_idx_class1 = df_class1['Accuracy'].idxmax()\n",
    "        else:\n",
    "            best_accuracy_class1 = df_class1.iloc[0, 0]  # Assume first column is accuracy\n",
    "            best_idx_class1 = 0\n",
    "        \n",
    "        # Get best training distances\n",
    "        train_distances_class0 = df_class0.iloc[best_idx_class0]\n",
    "        train_distances_class1 = df_class1.iloc[best_idx_class1]\n",
    "        \n",
    "        print(f\"Best Class 0 accuracy: {best_accuracy_class0:.4f} ({best_accuracy_class0*100:.2f}%)\")\n",
    "        print(f\"Best Class 1 accuracy: {best_accuracy_class1:.4f} ({best_accuracy_class1*100:.2f}%)\")\n",
    "        \n",
    "        # Distance measures\n",
    "        distance_measures = [\n",
    "            'Anderson_Darling_dist', 'CVM_dist', 'DTS_dist',\n",
    "            'Kolmogorov_Smirnov_dist', 'Kuiper_dist', 'Wasserstein_dist'\n",
    "        ]\n",
    "        \n",
    "        # Calculate ratios and similarities for each class\n",
    "        class0_analysis = analyze_class_distances(\n",
    "            train_distances_class0, realtime_distances_class0, \n",
    "            distance_measures, \"Class 0\", best_accuracy_class0\n",
    "        )\n",
    "        \n",
    "        class1_analysis = analyze_class_distances(\n",
    "            train_distances_class1, realtime_distances_class1, \n",
    "            distance_measures, \"Class 1\", best_accuracy_class1\n",
    "        )\n",
    "        \n",
    "        # Calculate overall similarity score\n",
    "        overall_similarity = calculate_weighted_similarity(class0_analysis, class1_analysis)\n",
    "        \n",
    "        # Estimate final accuracy\n",
    "        overall_best_accuracy = max(best_accuracy_class0, best_accuracy_class1)\n",
    "        estimated_accuracy = estimate_final_accuracy(overall_similarity, overall_best_accuracy)\n",
    "        \n",
    "        # Prepare detailed results\n",
    "        detailed_analysis = {\n",
    "            'class0_analysis': class0_analysis,\n",
    "            'class1_analysis': class1_analysis,\n",
    "            'overall_similarity': overall_similarity,\n",
    "            'best_accuracy_class0': best_accuracy_class0,\n",
    "            'best_accuracy_class1': best_accuracy_class1,\n",
    "            'overall_best_accuracy': overall_best_accuracy,\n",
    "            'estimated_accuracy': estimated_accuracy\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n🎯 Final Results:\")\n",
    "        print(f\"Overall similarity score: {overall_similarity:.2f}%\")\n",
    "        print(f\"Estimated accuracy: {estimated_accuracy:.4f} ({estimated_accuracy*100:.2f}%)\")\n",
    "        \n",
    "        return estimated_accuracy, detailed_analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in accuracy estimation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return 0.5, {}\n",
    "\n",
    "def analyze_class_distances(train_distances, realtime_distances, distance_measures, class_name, class_accuracy):\n",
    "    \"\"\"\n",
    "    Analyze distances for a specific class and calculate similarity.\n",
    "    \"\"\"\n",
    "    print(f\"\\n📏 {class_name} Distance Analysis:\")\n",
    "    print(f\"{'Measure':<25} {'Training':<12} {'Real-time':<12} {'Ratio':<8} {'Similarity %':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    similarities = []\n",
    "    valid_measures = 0\n",
    "    \n",
    "    for measure in distance_measures:\n",
    "        try:\n",
    "            if measure in train_distances.index and measure in realtime_distances:\n",
    "                train_val = train_distances[measure]\n",
    "                real_val = realtime_distances[measure]\n",
    "                \n",
    "                if pd.isna(train_val) or train_val == 0:\n",
    "                    continue\n",
    "                \n",
    "                ratio = real_val / train_val\n",
    "                \n",
    "                # Calculate similarity based on ratio\n",
    "                # Since ratio < 1 means real-time distances are smaller (better similarity)\n",
    "                if ratio <= 1.0:\n",
    "                    similarity = 100 * (2 - ratio)  # Higher similarity for lower ratios\n",
    "                    if similarity > 100:\n",
    "                        similarity = 100\n",
    "                else:\n",
    "                    similarity = 100 / ratio  # Lower similarity for higher ratios\n",
    "                \n",
    "                similarities.append(similarity)\n",
    "                valid_measures += 1\n",
    "                \n",
    "                print(f\"{measure:<25} {train_val:<12.6f} {real_val:<12.6f} {ratio:<8.2f} {similarity:<12.2f}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {measure}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    avg_similarity = np.mean(similarities) if similarities else 0\n",
    "    print(f\"\\nAverage similarity for {class_name}: {avg_similarity:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'similarities': similarities,\n",
    "        'average_similarity': avg_similarity,\n",
    "        'valid_measures': valid_measures,\n",
    "        'class_accuracy': class_accuracy\n",
    "    }\n",
    "\n",
    "def calculate_weighted_similarity(class0_analysis, class1_analysis):\n",
    "    \"\"\"\n",
    "    Calculate weighted overall similarity based on both classes.\n",
    "    \"\"\"\n",
    "    total_weighted_similarity = 0\n",
    "    total_weight = 0\n",
    "    \n",
    "    # Weight by number of valid measures and class accuracy\n",
    "    for analysis in [class0_analysis, class1_analysis]:\n",
    "        if analysis['valid_measures'] > 0:\n",
    "            weight = analysis['valid_measures'] * analysis['class_accuracy']\n",
    "            total_weighted_similarity += analysis['average_similarity'] * weight\n",
    "            total_weight += weight\n",
    "    \n",
    "    if total_weight > 0:\n",
    "        overall_similarity = total_weighted_similarity / total_weight\n",
    "    else:\n",
    "        overall_similarity = 50.0  # Default if no valid measures\n",
    "    \n",
    "    return overall_similarity\n",
    "\n",
    "def estimate_final_accuracy(similarity_score, best_training_accuracy):\n",
    "    \"\"\"\n",
    "    Estimate final accuracy based on similarity score.\n",
    "    \"\"\"\n",
    "    # Define confidence levels and retention factors\n",
    "    if similarity_score >= 95.0:\n",
    "        retention_factor = 0.98\n",
    "        confidence = \"EXCELLENT\"\n",
    "    elif similarity_score >= 90.0:\n",
    "        retention_factor = 0.95\n",
    "        confidence = \"VERY_HIGH\"\n",
    "    elif similarity_score >= 85.0:\n",
    "        retention_factor = 0.90\n",
    "        confidence = \"HIGH\"\n",
    "    elif similarity_score >= 80.0:\n",
    "        retention_factor = 0.85\n",
    "        confidence = \"GOOD\"\n",
    "    elif similarity_score >= 75.0:\n",
    "        retention_factor = 0.80\n",
    "        confidence = \"MODERATE\"\n",
    "    elif similarity_score >= 70.0:\n",
    "        retention_factor = 0.75\n",
    "        confidence = \"FAIR\"\n",
    "    else:\n",
    "        retention_factor = 0.65\n",
    "        confidence = \"LOW\"\n",
    "    \n",
    "    estimated_accuracy = best_training_accuracy * retention_factor\n",
    "    \n",
    "    print(f\"\\nConfidence level: {confidence}\")\n",
    "    print(f\"Retention factor: {retention_factor:.2f}\")\n",
    "    \n",
    "    return estimated_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a7105396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Loading training data and calculating accuracy estimation...\n",
      "Best Class 0 accuracy: 0.9980 (99.80%)\n",
      "Best Class 1 accuracy: 0.9980 (99.80%)\n",
      "\n",
      "📏 Class 0 Distance Analysis:\n",
      "Measure                   Training     Real-time    Ratio    Similarity %\n",
      "--------------------------------------------------------------------------------\n",
      "Anderson_Darling_dist     0.025108     0.012789     0.51     100.00      \n",
      "CVM_dist                  1.569996     0.678271     0.43     100.00      \n",
      "DTS_dist                  0.284515     0.225784     0.79     100.00      \n",
      "Kolmogorov_Smirnov_dist   0.007785     0.003745     0.48     100.00      \n",
      "Kuiper_dist               0.010759     0.005282     0.49     100.00      \n",
      "Wasserstein_dist          10.844416    10.033164    0.93     100.00      \n",
      "\n",
      "Average similarity for Class 0: 100.00%\n",
      "\n",
      "📏 Class 1 Distance Analysis:\n",
      "Measure                   Training     Real-time    Ratio    Similarity %\n",
      "--------------------------------------------------------------------------------\n",
      "Anderson_Darling_dist     0.006444     0.005085     0.79     100.00      \n",
      "CVM_dist                  0.119378     0.077364     0.65     100.00      \n",
      "DTS_dist                  0.294817     0.276289     0.94     100.00      \n",
      "Kolmogorov_Smirnov_dist   0.011688     0.006717     0.57     100.00      \n",
      "Kuiper_dist               0.011707     0.007309     0.62     100.00      \n",
      "Wasserstein_dist          3.220949     2.419690     0.75     100.00      \n",
      "\n",
      "Average similarity for Class 1: 100.00%\n",
      "\n",
      "Confidence level: EXCELLENT\n",
      "Retention factor: 0.98\n",
      "\n",
      "🎯 Final Results:\n",
      "Overall similarity score: 100.00%\n",
      "Estimated accuracy: 0.9781 (97.81%)\n"
     ]
    }
   ],
   "source": [
    "estimated_accuracy, analysis = estimate_accuracy_from_distance_ratios(\n",
    "        training_data_path_class0=\"Class0.xlsx\",\n",
    "        training_data_path_class1=\"Class1.xlsx\",\n",
    "        realtime_distances_class0=realtime_distances_class0,\n",
    "        realtime_distances_class1=realtime_distances_class1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "62a3dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safeml_confidence_assessment_both_classes(best_training_accuracy, estimated_accuracy, distance_analysis):\n",
    "    \"\"\"\n",
    "    Assess confidence using SafeML methodology based on BOTH Class 0 and Class 1 statistical distances.\n",
    "    Uses comprehensive analysis of all distance measures for decision making.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    best_training_accuracy : float\n",
    "        Best accuracy achieved during training\n",
    "    estimated_accuracy : float\n",
    "        Estimated accuracy based on both classes' distances\n",
    "    distance_analysis : dict\n",
    "        Detailed analysis of distance differences for both classes\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (decision, confidence_level, accuracy_difference)\n",
    "    \"\"\"\n",
    "    # Calculate accuracy difference\n",
    "    accuracy_difference = abs(best_training_accuracy - estimated_accuracy)\n",
    "    overall_similarity = distance_analysis.get('overall_similarity', 0.0)\n",
    "    overall_diff_percent = 100 - overall_similarity\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*80)\n",
    "    print(f\"SafeML Confidence Assessment (Both Classes):\")\n",
    "    print(f\"   Best training accuracy: {best_training_accuracy:.4f} ({best_training_accuracy*100:.2f}%)\")\n",
    "    print(f\"   Estimated accuracy: {estimated_accuracy:.4f} ({estimated_accuracy*100:.2f}%)\")\n",
    "    print(f\"   Accuracy difference: {accuracy_difference:.4f} ({accuracy_difference*100:.2f}%)\")\n",
    "    print(f\"   Overall similarity score: {overall_similarity:.2f}%\")\n",
    "    print(f\"   Overall distance difference: {overall_diff_percent:.2f}%\")\n",
    "    \n",
    "    # Multi-criteria decision making\n",
    "    decision_factors = []\n",
    "    \n",
    "    # Factor 1: Overall similarity threshold\n",
    "    if overall_similarity >= 90.0:\n",
    "        similarity_decision = \"AUTONOMOUS\"\n",
    "        similarity_confidence = \"HIGH\"\n",
    "        decision_factors.append((\"High Similarity\", \"AUTONOMOUS\", \"HIGH\"))\n",
    "    elif overall_similarity >= 75.0:\n",
    "        similarity_decision = \"AUTONOMOUS\"\n",
    "        similarity_confidence = \"MODERATE\"\n",
    "        decision_factors.append((\"Moderate Similarity\", \"AUTONOMOUS\", \"MODERATE\"))\n",
    "    else:\n",
    "        similarity_decision = \"HUMAN_INTERVENTION\"\n",
    "        similarity_confidence = \"LOW\"\n",
    "        decision_factors.append((\"Low Similarity\", \"HUMAN_INTERVENTION\", \"LOW\"))\n",
    "    \n",
    "    # Factor 2: Accuracy drop threshold\n",
    "    accuracy_drop_percent = (accuracy_difference / best_training_accuracy) * 100\n",
    "    if accuracy_drop_percent <= 5.0:\n",
    "        accuracy_decision = \"AUTONOMOUS\"\n",
    "        accuracy_confidence = \"HIGH\"\n",
    "        decision_factors.append((\"Small Accuracy Drop\", \"AUTONOMOUS\", \"HIGH\"))\n",
    "    elif accuracy_drop_percent <= 15.0:\n",
    "        accuracy_decision = \"AUTONOMOUS\"\n",
    "        accuracy_confidence = \"MODERATE\"\n",
    "        decision_factors.append((\"Moderate Accuracy Drop\", \"AUTONOMOUS\", \"MODERATE\"))\n",
    "    else:\n",
    "        accuracy_decision = \"HUMAN_INTERVENTION\"\n",
    "        accuracy_confidence = \"LOW\"\n",
    "        decision_factors.append((\"Large Accuracy Drop\", \"HUMAN_INTERVENTION\", \"LOW\"))\n",
    "    \n",
    "    # Factor 3: Class-specific analysis\n",
    "    class0_analysis = distance_analysis.get('class0_analysis', {})\n",
    "    class1_analysis = distance_analysis.get('class1_analysis', {})\n",
    "    \n",
    "    # Analyze Class 0\n",
    "    if class0_analysis and class0_analysis.get('valid_measures', 0) > 0:\n",
    "        avg_similarity_class0 = class0_analysis['average_similarity']\n",
    "        if avg_similarity_class0 >= 90.0:\n",
    "            class0_decision = \"AUTONOMOUS\"\n",
    "            class0_conf = \"HIGH\"\n",
    "        elif avg_similarity_class0 >= 75.0:\n",
    "            class0_decision = \"AUTONOMOUS\"\n",
    "            class0_conf = \"MODERATE\"\n",
    "        else:\n",
    "            class0_decision = \"HUMAN_INTERVENTION\"\n",
    "            class0_conf = \"LOW\"\n",
    "        decision_factors.append((\"Class 0 Analysis\", class0_decision, class0_conf))\n",
    "    \n",
    "    # Analyze Class 1\n",
    "    if class1_analysis and class1_analysis.get('valid_measures', 0) > 0:\n",
    "        avg_similarity_class1 = class1_analysis['average_similarity']\n",
    "        if avg_similarity_class1 >= 90.0:\n",
    "            class1_decision = \"AUTONOMOUS\"\n",
    "            class1_conf = \"HIGH\"\n",
    "        elif avg_similarity_class1 >= 75.0:\n",
    "            class1_decision = \"AUTONOMOUS\"\n",
    "            class1_conf = \"MODERATE\"\n",
    "        else:\n",
    "            class1_decision = \"HUMAN_INTERVENTION\"\n",
    "            class1_conf = \"LOW\"\n",
    "        decision_factors.append((\"Class 1 Analysis\", class1_decision, class1_conf))\n",
    "    \n",
    "    print(f\"\\nDecision Factors Analysis:\")\n",
    "    for factor_name, factor_decision, factor_confidence in decision_factors:\n",
    "        print(f\"   {factor_name:<25}: {factor_decision:<20} ({factor_confidence} confidence)\")\n",
    "    \n",
    "    # Final decision based on majority vote and severity\n",
    "    autonomous_votes = sum(1 for _, decision, _ in decision_factors if decision == \"AUTONOMOUS\")\n",
    "    intervention_votes = sum(1 for _, decision, _ in decision_factors if decision == \"HUMAN_INTERVENTION\")\n",
    "    \n",
    "    # High confidence votes have more weight\n",
    "    high_conf_autonomous = sum(1 for _, decision, conf in decision_factors \n",
    "                              if decision == \"AUTONOMOUS\" and conf == \"HIGH\")\n",
    "    high_conf_intervention = sum(1 for _, decision, conf in decision_factors \n",
    "                                if decision == \"HUMAN_INTERVENTION\" and conf == \"HIGH\")\n",
    "    \n",
    "    # Conservative approach: if any high-confidence intervention vote, lean towards intervention\n",
    "    if high_conf_intervention > 0:\n",
    "        final_decision = \"HUMAN_INTERVENTION\"\n",
    "        final_confidence = \"LOW\"\n",
    "        primary_reason = \"High-confidence factors indicate potential issues\"\n",
    "    elif high_conf_autonomous >= 2:\n",
    "        final_decision = \"AUTONOMOUS\"\n",
    "        final_confidence = \"HIGH\"\n",
    "        primary_reason = \"Multiple high-confidence factors support autonomous operation\"\n",
    "    elif autonomous_votes > intervention_votes:\n",
    "        final_decision = \"AUTONOMOUS\"\n",
    "        final_confidence = \"MODERATE\"\n",
    "        primary_reason = \"Majority of factors support autonomous operation\"\n",
    "    else:\n",
    "        final_decision = \"HUMAN_INTERVENTION\"\n",
    "        final_confidence = \"LOW\"\n",
    "        primary_reason = \"Majority of factors indicate potential issues\"\n",
    "    \n",
    "    # Determine actions and messages\n",
    "    if final_decision == \"AUTONOMOUS\":\n",
    "        if final_confidence == \"HIGH\":\n",
    "            message = \"All indicators support autonomous operation - System highly reliable\"\n",
    "            action = \"Continue automatic anomaly detection with normal monitoring\"\n",
    "        else:\n",
    "            message = \"System can operate autonomously with increased monitoring\"\n",
    "            action = \"Continue automatic detection with enhanced logging and periodic checks\"\n",
    "    else:\n",
    "        if final_confidence == \"LOW\":\n",
    "            message = \"Multiple indicators suggest reliability issues - Human intervention required\"\n",
    "            action = \"Alert security team for immediate manual review and validation\"\n",
    "        else:\n",
    "            message = \"Some reliability concerns detected - Recommend human oversight\"\n",
    "            action = \"Continue with manual validation of anomaly detections\"\n",
    "    \n",
    "    print(f\"\\n🎯 Final SafeML Decision:\")\n",
    "    print(f\"   Decision: {final_decision}\")\n",
    "    print(f\"   Confidence Level: {final_confidence}\")\n",
    "    print(f\"   Primary Reason: {primary_reason}\")\n",
    "    print(f\"   Message: {message}\")\n",
    "    print(f\"   Recommended Action: {action}\")\n",
    "    \n",
    "    # Additional metrics for transparency\n",
    "    print(f\"\\n📊 Decision Metrics:\")\n",
    "    print(f\"   Autonomous votes: {autonomous_votes}/{len(decision_factors)}\")\n",
    "    print(f\"   Intervention votes: {intervention_votes}/{len(decision_factors)}\")\n",
    "    print(f\"   High-confidence autonomous: {high_conf_autonomous}\")\n",
    "    print(f\"   High-confidence intervention: {high_conf_intervention}\")\n",
    "    print(f\"   Accuracy drop: {accuracy_drop_percent:.1f}%\")\n",
    "    print(f\"   Overall similarity: {overall_similarity:.1f}%\")\n",
    "    \n",
    "    print(f\"\\n📋 Decision Basis: Comprehensive analysis of both Class 0 and Class 1 statistical distances\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return final_decision, final_confidence, accuracy_difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f36ddf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SafeML Confidence Assessment (Both Classes):\n",
      "   Best training accuracy: 0.9980 (99.80%)\n",
      "   Estimated accuracy: 0.9781 (97.81%)\n",
      "   Accuracy difference: 0.0200 (2.00%)\n",
      "   Overall similarity score: 100.00%\n",
      "   Overall distance difference: -0.00%\n",
      "\n",
      "Decision Factors Analysis:\n",
      "   High Similarity          : AUTONOMOUS           (HIGH confidence)\n",
      "   Small Accuracy Drop      : AUTONOMOUS           (HIGH confidence)\n",
      "   Class 0 Analysis         : AUTONOMOUS           (HIGH confidence)\n",
      "   Class 1 Analysis         : AUTONOMOUS           (HIGH confidence)\n",
      "\n",
      "🎯 Final SafeML Decision:\n",
      "   Decision: AUTONOMOUS\n",
      "   Confidence Level: HIGH\n",
      "   Primary Reason: Multiple high-confidence factors support autonomous operation\n",
      "   Message: All indicators support autonomous operation - System highly reliable\n",
      "   Recommended Action: Continue automatic anomaly detection with normal monitoring\n",
      "\n",
      "📊 Decision Metrics:\n",
      "   Autonomous votes: 4/4\n",
      "   Intervention votes: 0/4\n",
      "   High-confidence autonomous: 4\n",
      "   High-confidence intervention: 0\n",
      "   Accuracy drop: 2.0%\n",
      "   Overall similarity: 100.0%\n",
      "\n",
      "📋 Decision Basis: Comprehensive analysis of both Class 0 and Class 1 statistical distances\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "best_training_accuracy =analysis['overall_best_accuracy']\n",
    "    \n",
    "    # Step 3: Make SafeML decision\n",
    "decision, confidence_level, accuracy_difference = safeml_confidence_assessment_both_classes(\n",
    "        best_training_accuracy=best_training_accuracy,\n",
    "        estimated_accuracy=estimated_accuracy,\n",
    "        distance_analysis=analysis\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomaly",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
